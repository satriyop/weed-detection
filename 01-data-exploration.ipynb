{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3a23b2",
   "metadata": {},
   "source": [
    "# 01 — Data Exploration: Weed Detection Datasets for Indonesian Rice Fields\n",
    "\n",
    "**Purpose:** Understand the datasets we'll use for training before touching any models.  \n",
    "**Runtime:** CPU only — no GPU needed. Save your GPU hours for training notebooks.  \n",
    "**Platform:** Works on both Kaggle and Google Colab.\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "1. **Platform detection** — auto-detect Kaggle vs Colab and set paths accordingly\n",
    "2. **Dataset switcher** — configure which dataset to explore (YOLO detection, folder classification, or mask segmentation)\n",
    "3. **Dataset exploration** — class distribution, sample images, image properties, data quality\n",
    "4. **Segmentation survey** — RiceSEG for pixel-level weed masks (D2)\n",
    "5. **Key observations** — what we learned that affects training decisions\n",
    "\n",
    "### Why Not DeepWeeds?\n",
    "\n",
    "The original notebook used **DeepWeeds** (Australian rangeland weeds, 9 classes). We replaced it because:\n",
    "- **Zero species overlap** with Indonesian rice paddy weeds\n",
    "- Australian rangeland ecology is fundamentally different from tropical rice fields\n",
    "- Training on irrelevant species teaches the model features that don't transfer\n",
    "\n",
    "### Recommended Datasets\n",
    "\n",
    "| Dataset | Task | Role | Source |\n",
    "|---------|------|------|--------|\n",
    "| **Crop & Weed Detection** (YOLO) | Object Detection | Pipeline learning, YOLO format | Kaggle (already available) |\n",
    "| **Bangladesh Rice Field Weed** | Classification | 11 tropical rice weed species | Mendeley Data (upload to Kaggle) |\n",
    "| **RiceSEG** | Segmentation | Pixel-level weed masks in rice fields | HuggingFace (upload to Kaggle) |\n",
    "\n",
    "This notebook explores the **detection** and **classification** datasets. Notebook 02 covers **RiceSEG** (segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e4d1a",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Platform Detection & Setup\n",
    "\n",
    "Since we want this notebook to run on **both Kaggle and Colab**, we detect the platform and set file paths accordingly.\n",
    "\n",
    "**How it works:**\n",
    "- Kaggle notebooks have `/kaggle/input/` directory\n",
    "- Colab notebooks have `google.colab` module available\n",
    "- If neither → running locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ce961",
   "metadata": {},
   "source": [
    "### Install Dependencies\n",
    "\n",
    "On Kaggle and Colab, pip installs are **ephemeral** — they disappear when the session restarts.  \n",
    "That's why every notebook starts with an install cell. This is normal for cloud notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2094ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform detected: local\n",
      "Python version: 3.14.2 (main, Dec  5 2025, 16:49:16) [Clang 17.0.0 (clang-1700.6.3.2)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Platform Detection ---\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "IS_LOCAL = not IS_KAGGLE and not IS_COLAB\n",
    "\n",
    "PLATFORM = 'kaggle' if IS_KAGGLE else ('colab' if IS_COLAB else 'local')\n",
    "print(f'Platform detected: {PLATFORM}')\n",
    "print(f'Python version: {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1fc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies ready.\n"
     ]
    }
   ],
   "source": [
    "# These are lightweight — no ML frameworks needed for exploration\n",
    "# matplotlib and pandas are pre-installed on both platforms\n",
    "import subprocess\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'Pillow'])\n",
    "\n",
    "print('Dependencies ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5212f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Consistent plot style\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print('Imports ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac001fbb",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Dataset Configuration\n",
    "\n",
    "This notebook supports multiple datasets through a **switcher pattern**. Change `ACTIVE_DATASET` to explore a different dataset — all analysis cells adapt automatically.\n",
    "\n",
    "### Available Datasets\n",
    "\n",
    "| Key | Dataset | Format | Status |\n",
    "|-----|---------|--------|--------|\n",
    "| `crop_weed_yolo` | Crop & Weed Detection | YOLO (images + `.txt` annotations) | On Kaggle — ready to use |\n",
    "| `bangladesh_rice_weed` | Bangladesh Rice Field Weed | Folder-based (one folder per class) | Upload from Mendeley Data |\n",
    "\n",
    "**Default:** `crop_weed_yolo` — works immediately on Kaggle with no extra setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72aaa3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active dataset: Crop & Weed Detection (YOLO)\n",
      "Format: yolo\n",
      "Task: detection\n",
      "\n",
      "Description: 2-class detection dataset (crop vs weed) with YOLO-format bounding boxes. Good for learning the YOLO annotation pipeline and detection basics.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATASET CONFIGURATION — Change ACTIVE_DATASET to switch\n",
    "# ============================================================\n",
    "\n",
    "DATASET_CONFIGS = {\n",
    "    'crop_weed_yolo': {\n",
    "        'name': 'Crop & Weed Detection (YOLO)',\n",
    "        'format': 'yolo',          # YOLO annotation format\n",
    "        'task': 'detection',\n",
    "        'kaggle_slug': 'crop-and-weed-detection-data-with-bounding-boxes',\n",
    "        'paths': {\n",
    "            'kaggle': '/kaggle/input/crop-and-weed-detection-data-with-bounding-boxes',\n",
    "            'colab': '/content/crop_weed_yolo',\n",
    "            'local': './data/crop_weed_yolo',\n",
    "        },\n",
    "        'description': (\n",
    "            '2-class detection dataset (crop vs weed) with YOLO-format bounding boxes. '\n",
    "            'Good for learning the YOLO annotation pipeline and detection basics.'\n",
    "        ),\n",
    "    },\n",
    "    'bangladesh_rice_weed': {\n",
    "        'name': 'Bangladesh Rice Field Weed',\n",
    "        'format': 'folder',         # One subfolder per class\n",
    "        'task': 'classification',\n",
    "        'kaggle_slug': None,        # Must upload from Mendeley\n",
    "        'paths': {\n",
    "            'kaggle': '/kaggle/input/bangladesh-rice-field-weed',\n",
    "            'colab': '/content/bangladesh_rice_weed',\n",
    "            'local': './data/bangladesh_rice_weed',\n",
    "        },\n",
    "        'description': (\n",
    "            '11 rice weed species from Bangladesh (tropical climate, closest to Indonesia). '\n",
    "            'Requires manual upload from Mendeley Data: '\n",
    "            'https://data.mendeley.com/datasets/mt72bmxz73/4'\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "# >>> CHANGE THIS to switch datasets <<<\n",
    "ACTIVE_DATASET = 'crop_weed_yolo'\n",
    "\n",
    "config = DATASET_CONFIGS[ACTIVE_DATASET]\n",
    "print(f'Active dataset: {config[\"name\"]}')\n",
    "print(f'Format: {config[\"format\"]}')\n",
    "print(f'Task: {config[\"task\"]}')\n",
    "print(f'\\nDescription: {config[\"description\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99d32c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Dataset Catalog\n",
    "\n",
    "### Crop & Weed Detection (YOLO)\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Classes** | 2 (crop, weed) |\n",
    "| **Format** | YOLO — each image has a `.txt` annotation with bounding boxes |\n",
    "| **Source** | Kaggle (search: \"crop and weed detection\") |\n",
    "| **Task** | Object detection |\n",
    "| **Annotation** | `class_id center_x center_y width height` (normalized 0-1) |\n",
    "\n",
    "### Bangladesh Rice Field Weed\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Classes** | 11 rice weed species |\n",
    "| **Format** | Folder-based — one subdirectory per species |\n",
    "| **Source** | Mendeley Data (NOT on Kaggle — must upload as private dataset) |\n",
    "| **Task** | Image classification |\n",
    "| **Climate** | Tropical (Bangladesh) — closest match to Indonesian rice fields |\n",
    "| **Species** | *Cyperus difformis*, *Echinochloa crus-galli*, *Fimbristylis miliacea*, etc. |\n",
    "\n",
    "### Why These Datasets?\n",
    "\n",
    "- **Crop & Weed Detection** — Already on Kaggle, zero setup, teaches YOLO annotation format\n",
    "- **Bangladesh Rice Weed** — 11 species from tropical rice fields, directly relevant to Indonesia\n",
    "- Both are better than DeepWeeds (Australian rangeland, zero species overlap with Indonesian rice)\n",
    "\n",
    "> **Note:** For segmentation (pixel-level masks), see **notebook 02** which explores RiceSEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc442d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root: data/crop_weed_yolo\n",
      "Exists: False\n",
      "\n",
      "============================================================\n",
      "DATASET NOT FOUND — Setup Instructions\n",
      "============================================================\n",
      "On Kaggle:\n",
      "  1. Click \"Add Data\" in the sidebar\n",
      "  2. Search: \"crop and weed detection\"\n",
      "  3. Add the dataset by ravirajsinh45\n",
      "  4. Re-run this cell\n",
      "\n",
      "On Colab:\n",
      "  1. Download from Kaggle: kaggle datasets download -d ravirajsinh45/crop-and-weed-detection-data-with-bounding-boxes\n",
      "  2. Unzip to data/crop_weed_yolo\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Set dataset path based on platform ---\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    DATA_ROOT = Path(config['paths']['kaggle'])\n",
    "elif IS_COLAB:\n",
    "    DATA_ROOT = Path(config['paths']['colab'])\n",
    "else:\n",
    "    DATA_ROOT = Path(config['paths']['local'])\n",
    "\n",
    "print(f'Data root: {DATA_ROOT}')\n",
    "print(f'Exists: {DATA_ROOT.exists()}')\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    print()\n",
    "    print('=' * 60)\n",
    "    print('DATASET NOT FOUND — Setup Instructions')\n",
    "    print('=' * 60)\n",
    "    if config['format'] == 'yolo':\n",
    "        print(f'On Kaggle:')\n",
    "        print(f'  1. Click \"Add Data\" in the sidebar')\n",
    "        print(f'  2. Search: \"crop and weed detection\"')\n",
    "        print(f'  3. Add the dataset by ravirajsinh45')\n",
    "        print(f'  4. Re-run this cell')\n",
    "        print()\n",
    "        print(f'On Colab:')\n",
    "        print(f'  1. Download from Kaggle: kaggle datasets download -d ravirajsinh45/{config[\"kaggle_slug\"]}')\n",
    "        print(f'  2. Unzip to {DATA_ROOT}')\n",
    "    elif config['format'] == 'folder':\n",
    "        print(f'This dataset is NOT on Kaggle. You must upload it manually:')\n",
    "        print(f'  1. Download from Mendeley Data:')\n",
    "        print(f'     https://data.mendeley.com/datasets/mt72bmxz73/4')\n",
    "        print(f'  2. On Kaggle: New Dataset > upload the extracted folder')\n",
    "        print(f'  3. Name it \"bangladesh-rice-field-weed\"')\n",
    "        print(f'  4. Attach it to this notebook via \"Add Data\"')\n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa41cb2",
   "metadata": {},
   "source": [
    "### Dataset Structure Discovery\n",
    "\n",
    "Before loading anything, let's see what files and folders exist.  \n",
    "Different datasets have different structures — YOLO has `.txt` annotation files alongside images, while folder-based datasets organize images into class subdirectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ac4c2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data root does not exist: data/crop_weed_yolo\n",
      "Follow the setup instructions above for your platform.\n"
     ]
    }
   ],
   "source": [
    "# List top-level contents\n",
    "if DATA_ROOT.exists():\n",
    "    contents = sorted(DATA_ROOT.iterdir())\n",
    "    print(f'Contents of {DATA_ROOT}:')\n",
    "    for item in contents[:30]:  # Show first 30 items\n",
    "        kind = 'DIR' if item.is_dir() else f'FILE ({item.suffix})'\n",
    "        size = item.stat().st_size if item.is_file() else ''\n",
    "        size_str = f' — {size / 1024:.1f} KB' if size else ''\n",
    "        print(f'  {kind}: {item.name}{size_str}')\n",
    "    \n",
    "    if len(contents) > 30:\n",
    "        print(f'  ... and {len(contents) - 30} more items')\n",
    "    \n",
    "    # Count file types\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    all_images = [f for f in DATA_ROOT.rglob('*') if f.suffix.lower() in image_extensions]\n",
    "    all_txts = [f for f in DATA_ROOT.rglob('*.txt')]\n",
    "    all_csvs = [f for f in DATA_ROOT.rglob('*.csv')]\n",
    "    all_xmls = [f for f in DATA_ROOT.rglob('*.xml')]\n",
    "    \n",
    "    print(f'\\nFile type summary:')\n",
    "    print(f'  Images (.jpg/.jpeg/.png/.bmp): {len(all_images)}')\n",
    "    print(f'  Text files (.txt):             {len(all_txts)}')\n",
    "    print(f'  CSV files (.csv):              {len(all_csvs)}')\n",
    "    print(f'  XML files (.xml):              {len(all_xmls)}')\n",
    "    \n",
    "    # Show which directories contain images\n",
    "    image_dirs = Counter(str(f.parent.relative_to(DATA_ROOT)) for f in all_images)\n",
    "    print(f'\\nImage directories: {dict(image_dirs)}')\n",
    "else:\n",
    "    print(f'Data root does not exist: {DATA_ROOT}')\n",
    "    print('Follow the setup instructions above for your platform.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d829250",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "The loading strategy depends on the dataset format:\n",
    "- **YOLO format:** Parse `.txt` annotation files alongside images\n",
    "- **Folder format:** Walk subdirectories, each directory name = class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOLO annotation parser ---\n",
    "\n",
    "def parse_yolo_annotation(txt_path):\n",
    "    \"\"\"Parse a YOLO annotation file into a list of bounding boxes.\n",
    "    \n",
    "    Each line: class_id center_x center_y width height (all normalized 0-1)\n",
    "    Returns: list of dicts with keys: class_id, cx, cy, w, h\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    try:\n",
    "        with open(txt_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    boxes.append({\n",
    "                        'class_id': int(parts[0]),\n",
    "                        'cx': float(parts[1]),\n",
    "                        'cy': float(parts[2]),\n",
    "                        'w': float(parts[3]),\n",
    "                        'h': float(parts[4]),\n",
    "                    })\n",
    "    except Exception:\n",
    "        pass\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def load_classes_txt(data_root):\n",
    "    \"\"\"Load class names from classes.txt or similar files.\"\"\"\n",
    "    for candidate in ['classes.txt', 'obj.names', 'data.names']:\n",
    "        path = data_root / candidate\n",
    "        if not path.exists():\n",
    "            # Try one level deeper\n",
    "            for subdir in data_root.iterdir():\n",
    "                if subdir.is_dir():\n",
    "                    deeper = subdir / candidate\n",
    "                    if deeper.exists():\n",
    "                        path = deeper\n",
    "                        break\n",
    "        if path.exists():\n",
    "            with open(path) as f:\n",
    "                names = [line.strip() for line in f if line.strip()]\n",
    "            return {i: name for i, name in enumerate(names)}\n",
    "    \n",
    "    # Also try to find in any .txt that looks like a class list (short, no spaces per line)\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_yolo_dataset(data_root):\n",
    "    \"\"\"Load a YOLO-format dataset into a DataFrame.\n",
    "    \n",
    "    Returns DataFrame with columns: image_path, annotation_path, num_objects, class_ids\n",
    "    \"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    rows = []\n",
    "    \n",
    "    # Find all images\n",
    "    all_images = sorted(f for f in data_root.rglob('*') if f.suffix.lower() in image_extensions)\n",
    "    \n",
    "    for img_path in all_images:\n",
    "        # Look for matching .txt annotation\n",
    "        txt_path = img_path.with_suffix('.txt')\n",
    "        if not txt_path.exists():\n",
    "            # Try in a parallel directory (e.g., images/ vs labels/)\n",
    "            rel = img_path.relative_to(data_root)\n",
    "            parts = list(rel.parts)\n",
    "            for i, part in enumerate(parts):\n",
    "                if part.lower() in ('images', 'image', 'img'):\n",
    "                    parts[i] = 'labels'\n",
    "                    alt = data_root / Path(*parts)\n",
    "                    alt = alt.with_suffix('.txt')\n",
    "                    if alt.exists():\n",
    "                        txt_path = alt\n",
    "                        break\n",
    "        \n",
    "        boxes = parse_yolo_annotation(txt_path) if txt_path.exists() else []\n",
    "        class_ids = [b['class_id'] for b in boxes]\n",
    "        \n",
    "        rows.append({\n",
    "            'image_path': str(img_path),\n",
    "            'annotation_path': str(txt_path) if txt_path.exists() else None,\n",
    "            'num_objects': len(boxes),\n",
    "            'class_ids': class_ids,\n",
    "            'has_annotation': txt_path.exists(),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def load_folder_dataset(data_root):\n",
    "    \"\"\"Load a folder-based classification dataset into a DataFrame.\n",
    "    \n",
    "    Expects: data_root/class_name/*.jpg\n",
    "    Returns DataFrame with columns: image_path, class_name, label\n",
    "    \"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    rows = []\n",
    "    \n",
    "    # Find all subdirectories that contain images\n",
    "    subdirs = sorted(d for d in data_root.iterdir() if d.is_dir())\n",
    "    \n",
    "    # Check one level deeper if there's only one subdir\n",
    "    if len(subdirs) == 1:\n",
    "        deeper = subdirs[0]\n",
    "        deeper_subdirs = sorted(d for d in deeper.iterdir() if d.is_dir())\n",
    "        if len(deeper_subdirs) > 1:\n",
    "            subdirs = deeper_subdirs\n",
    "            print(f'  Found nested structure: using {deeper.name}/ as root')\n",
    "    \n",
    "    class_names = {}\n",
    "    for idx, subdir in enumerate(subdirs):\n",
    "        imgs = [f for f in subdir.iterdir() if f.suffix.lower() in image_extensions]\n",
    "        if imgs:\n",
    "            class_names[idx] = subdir.name\n",
    "            for img in imgs:\n",
    "                rows.append({\n",
    "                    'image_path': str(img),\n",
    "                    'class_name': subdir.name,\n",
    "                    'label': idx,\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rows), class_names\n",
    "\n",
    "\n",
    "print('Dataset loaders defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ec0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO dataset from data/crop_weed_yolo...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/crop_weed_yolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoading YOLO dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load class names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m CLASS_NAMES = \u001b[43mload_classes_txt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_ROOT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m CLASS_NAMES:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mClass names from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCLASS_NAMES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mload_classes_txt\u001b[39m\u001b[34m(data_root)\u001b[39m\n\u001b[32m     30\u001b[39m path = data_root / candidate\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Try one level deeper\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_root\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     34\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m subdir.is_dir():\n\u001b[32m     35\u001b[39m             deeper = subdir / candidate\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.2_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/pathlib/__init__.py:836\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m    831\u001b[39m \n\u001b[32m    832\u001b[39m \u001b[33;03mThe children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m    833\u001b[39m \u001b[33;03mspecial entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    835\u001b[39m root_dir = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[32m    837\u001b[39m     entries = \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m root_dir == \u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/crop_weed_yolo'"
     ]
    }
   ],
   "source": [
    "# --- Load the active dataset ---\n",
    "df = None\n",
    "CLASS_NAMES = {}\n",
    "\n",
    "if config['format'] == 'yolo':\n",
    "    print(f'Loading YOLO dataset from {DATA_ROOT}...')\n",
    "    \n",
    "    # Load class names\n",
    "    CLASS_NAMES = load_classes_txt(DATA_ROOT)\n",
    "    if CLASS_NAMES:\n",
    "        print(f'Class names from file: {CLASS_NAMES}')\n",
    "    else:\n",
    "        # Default for Crop & Weed Detection\n",
    "        CLASS_NAMES = {0: 'crop', 1: 'weed'}\n",
    "        print(f'Using default class names: {CLASS_NAMES}')\n",
    "    \n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "    df = load_yolo_dataset(DATA_ROOT)\n",
    "    \n",
    "    FNAME_COL = 'image_path'\n",
    "    LABEL_COL = 'class_ids'  # list of class IDs per image\n",
    "    \n",
    "    print(f'\\nLoaded {len(df)} images')\n",
    "    print(f'With annotations: {df[\"has_annotation\"].sum()}')\n",
    "    print(f'Without annotations: {(~df[\"has_annotation\"]).sum()}')\n",
    "    print(f'Total objects: {df[\"num_objects\"].sum()}')\n",
    "\n",
    "elif config['format'] == 'folder':\n",
    "    print(f'Loading folder-based dataset from {DATA_ROOT}...')\n",
    "    \n",
    "    df, CLASS_NAMES = load_folder_dataset(DATA_ROOT)\n",
    "    NUM_CLASSES = len(CLASS_NAMES)\n",
    "    \n",
    "    FNAME_COL = 'image_path'\n",
    "    LABEL_COL = 'label'\n",
    "    \n",
    "    print(f'\\nLoaded {len(df)} images across {NUM_CLASSES} classes:')\n",
    "    for idx, name in sorted(CLASS_NAMES.items()):\n",
    "        count = len(df[df['label'] == idx])\n",
    "        print(f'  {idx}: {name} — {count} images')\n",
    "\n",
    "if df is not None:\n",
    "    print(f'\\nDataFrame ready: {df.shape[0]} rows, {df.shape[1]} columns')\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print('Could not load dataset. Check DATA_ROOT and dataset structure above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5a6be48",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataFrame not loaded. Check the output of the previous cell.\nThe dataset structure may not match what this notebook expects.\nLook at the \"Dataset Structure Discovery\" output for clues.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m      3\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mDataFrame not loaded. Check the output of the previous cell.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThe dataset structure may not match what this notebook expects.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mLook at the \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset Structure Discovery\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m output for clues.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m     )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# --- Normalize and validate ---\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mDataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataFrame not loaded. Check the output of the previous cell.\nThe dataset structure may not match what this notebook expects.\nLook at the \"Dataset Structure Discovery\" output for clues."
     ]
    }
   ],
   "source": [
    "if df is None:\n",
    "    raise RuntimeError(\n",
    "        'DataFrame not loaded. Check the output of the previous cell.\\n'\n",
    "        'The dataset structure may not match what this notebook expects.\\n'\n",
    "        'Look at the \"Dataset Structure Discovery\" output for clues.'\n",
    "    )\n",
    "\n",
    "# --- Normalize and validate ---\n",
    "print(f'Dataset: {config[\"name\"]}')\n",
    "print(f'Format: {config[\"format\"]}')\n",
    "print(f'Rows: {len(df)}')\n",
    "print(f'Columns: {list(df.columns)}')\n",
    "print(f'Classes ({NUM_CLASSES}): {CLASS_NAMES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9bc02",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Class Distribution Analysis\n",
    "\n",
    "**Why this matters:** If classes are imbalanced (some have far more examples), the model will:\n",
    "- Perform well on majority classes (lots of examples to learn from)\n",
    "- Perform poorly on minority classes (too few examples)\n",
    "- Report misleadingly high accuracy (just by guessing the majority class)\n",
    "\n",
    "**What to look for:**\n",
    "- Are any classes severely underrepresented?\n",
    "- What's the ratio between largest and smallest class?\n",
    "- For YOLO: how many objects per image on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a8cb79b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[33m'\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33myolo\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Count objects per class across all images\u001b[39;00m\n\u001b[32m      5\u001b[39m     all_class_ids = []\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclass_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m      7\u001b[39m         all_class_ids.extend(ids)\n\u001b[32m      9\u001b[39m     class_counts_raw = Counter(all_class_ids)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Class distribution — adapts to dataset format\n",
    "\n",
    "if config['format'] == 'yolo':\n",
    "    # Count objects per class across all images\n",
    "    all_class_ids = []\n",
    "    for ids in df['class_ids']:\n",
    "        all_class_ids.extend(ids)\n",
    "    \n",
    "    class_counts_raw = Counter(all_class_ids)\n",
    "    class_counts = pd.Series({CLASS_NAMES.get(k, f'class_{k}'): v \n",
    "                              for k, v in sorted(class_counts_raw.items())})\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Objects per class\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, NUM_CLASSES))\n",
    "    bars = axes[0].barh(class_counts.index, class_counts.values, color=colors)\n",
    "    axes[0].set_xlabel('Number of Objects')\n",
    "    axes[0].set_title('Objects per Class')\n",
    "    for bar, count in zip(bars, class_counts.values):\n",
    "        axes[0].text(bar.get_width() + 20, bar.get_y() + bar.get_height()/2,\n",
    "                     f'{count:,}', va='center', fontsize=10)\n",
    "    \n",
    "    # Objects per image histogram\n",
    "    axes[1].hist(df['num_objects'], bins=range(0, df['num_objects'].max() + 2),\n",
    "                 color='steelblue', edgecolor='white')\n",
    "    axes[1].set_xlabel('Objects per Image')\n",
    "    axes[1].set_ylabel('Number of Images')\n",
    "    axes[1].set_title('Objects per Image Distribution')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[2].pie(class_counts.values, labels=class_counts.index, colors=colors,\n",
    "                autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
    "    axes[2].set_title('Class Proportions (by object count)')\n",
    "    \n",
    "    plt.suptitle(f'{config[\"name\"]} — Class Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Total images: {len(df):,}')\n",
    "    print(f'Total objects: {sum(class_counts.values):,}')\n",
    "    print(f'Mean objects per image: {df[\"num_objects\"].mean():.1f}')\n",
    "    print(f'Images with no objects: {(df[\"num_objects\"] == 0).sum()}')\n",
    "    if len(class_counts) > 1:\n",
    "        print(f'Imbalance ratio: {class_counts.max() / max(class_counts.min(), 1):.1f}x')\n",
    "\n",
    "elif config['format'] == 'folder':\n",
    "    class_counts = df['class_name'].value_counts().sort_index()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, NUM_CLASSES))\n",
    "    bars = axes[0].barh(class_counts.index, class_counts.values, color=colors[:len(class_counts)])\n",
    "    axes[0].set_xlabel('Number of Images')\n",
    "    axes[0].set_title('Images per Class')\n",
    "    for bar, count in zip(bars, class_counts.values):\n",
    "        axes[0].text(bar.get_width() + 5, bar.get_y() + bar.get_height()/2,\n",
    "                     f'{count:,}', va='center', fontsize=10)\n",
    "    \n",
    "    axes[1].pie(class_counts.values, labels=class_counts.index, colors=colors[:len(class_counts)],\n",
    "                autopct='%1.1f%%', startangle=90, textprops={'fontsize': 9})\n",
    "    axes[1].set_title('Class Proportions')\n",
    "    \n",
    "    plt.suptitle(f'{config[\"name\"]} — Class Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Total images: {len(df):,}')\n",
    "    print(f'Largest class: {class_counts.idxmax()} ({class_counts.max():,} images)')\n",
    "    print(f'Smallest class: {class_counts.idxmin()} ({class_counts.min():,} images)')\n",
    "    print(f'Imbalance ratio: {class_counts.max() / class_counts.min():.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b5f5c",
   "metadata": {},
   "source": [
    "### Interpreting the Distribution\n",
    "\n",
    "**For YOLO datasets:**\n",
    "- If one class dominates, the detector may ignore the minority class\n",
    "- Images with 0 objects are \"negative\" examples — useful but should not dominate\n",
    "- Many objects per image = dense scenes (harder detection task)\n",
    "\n",
    "**For folder-based datasets:**\n",
    "- Imbalance ratio >5x usually requires special handling (weighted loss, oversampling)\n",
    "- Very small classes (<100 images) will need heavy data augmentation\n",
    "- The confusion matrix in notebook 04 will reveal if imbalance causes prediction bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd5b25",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Sample Images\n",
    "\n",
    "**Why?** Visual inspection tells you things statistics can't:\n",
    "- Are images clear enough for a model to learn from?\n",
    "- Do some classes look visually similar?\n",
    "- Are there obviously mislabeled images?\n",
    "- What's the image quality and resolution?\n",
    "\n",
    "For YOLO datasets, we overlay bounding boxes on the images to verify annotation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eadc57b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FNAME_COL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     55\u001b[39m         ax.text(x1, y1 - \u001b[32m2\u001b[39m, label, fontsize=\u001b[32m7\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33mwhite\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     56\u001b[39m                 bbox=\u001b[38;5;28mdict\u001b[39m(boxstyle=\u001b[33m'\u001b[39m\u001b[33mround,pad=0.2\u001b[39m\u001b[33m'\u001b[39m, facecolor=color, alpha=\u001b[32m0.8\u001b[39m))\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Test: find the first image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m test_path = find_image_path(df[\u001b[43mFNAME_COL\u001b[49m].iloc[\u001b[32m0\u001b[39m])\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTest image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[FNAME_COL].iloc[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFound at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'FNAME_COL' is not defined"
     ]
    }
   ],
   "source": [
    "def find_image_path(filename):\n",
    "    \"\"\"Find the full path of an image file across different dataset structures.\"\"\"\n",
    "    p = Path(filename)\n",
    "    if p.is_absolute() and p.exists():\n",
    "        return p\n",
    "    \n",
    "    direct = DATA_ROOT / filename\n",
    "    if direct.exists():\n",
    "        return direct\n",
    "    \n",
    "    for subdir in ['images', 'train', 'data', 'img']:\n",
    "        candidate = DATA_ROOT / subdir / filename\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    \n",
    "    for parent in DATA_ROOT.iterdir():\n",
    "        if parent.is_dir():\n",
    "            candidate = parent / filename\n",
    "            if candidate.exists():\n",
    "                return candidate\n",
    "            for subdir in ['images', 'train']:\n",
    "                candidate = parent / subdir / filename\n",
    "                if candidate.exists():\n",
    "                    return candidate\n",
    "    \n",
    "    matches = list(DATA_ROOT.rglob(Path(filename).name))\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def draw_yolo_boxes(ax, img, boxes, class_names, colors=None):\n",
    "    \"\"\"Draw YOLO bounding boxes on a matplotlib axes.\"\"\"\n",
    "    h, w = img.shape[:2] if hasattr(img, 'shape') else img.size[::-1]\n",
    "    \n",
    "    if colors is None:\n",
    "        cmap = plt.cm.Set1\n",
    "        colors = {i: cmap(i / max(len(class_names), 1)) for i in class_names}\n",
    "    \n",
    "    for box in boxes:\n",
    "        cx, cy, bw, bh = box['cx'], box['cy'], box['w'], box['h']\n",
    "        x1 = (cx - bw / 2) * w\n",
    "        y1 = (cy - bh / 2) * h\n",
    "        rect_w = bw * w\n",
    "        rect_h = bh * h\n",
    "        \n",
    "        cid = box['class_id']\n",
    "        color = colors.get(cid, 'red')\n",
    "        label = class_names.get(cid, f'class_{cid}')\n",
    "        \n",
    "        rect = patches.Rectangle((x1, y1), rect_w, rect_h,\n",
    "                                  linewidth=2, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 2, label, fontsize=7, color='white',\n",
    "                bbox=dict(boxstyle='round,pad=0.2', facecolor=color, alpha=0.8))\n",
    "\n",
    "\n",
    "# Test: find the first image\n",
    "test_path = find_image_path(df[FNAME_COL].iloc[0])\n",
    "print(f'Test image: {df[FNAME_COL].iloc[0]}')\n",
    "print(f'Found at: {test_path}')\n",
    "if test_path is None:\n",
    "    print('\\nCould not find image file. Check DATA_ROOT structure above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18094e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample images — adapts to dataset format\n",
    "np.random.seed(42)\n",
    "\n",
    "if config['format'] == 'yolo':\n",
    "    # Show images WITH annotations, overlay bounding boxes\n",
    "    annotated = df[df['has_annotation'] & (df['num_objects'] > 0)]\n",
    "    if len(annotated) == 0:\n",
    "        annotated = df[df['has_annotation']]\n",
    "    \n",
    "    samples = annotated.sample(n=min(12, len(annotated)), random_state=42)\n",
    "    \n",
    "    rows_display = min(3, (len(samples) + 3) // 4)\n",
    "    cols_display = min(4, len(samples))\n",
    "    fig, axes = plt.subplots(rows_display, cols_display, \n",
    "                              figsize=(4 * cols_display, 4 * rows_display))\n",
    "    if rows_display == 1:\n",
    "        axes = [axes] if cols_display == 1 else [axes]\n",
    "    axes_flat = np.array(axes).flat\n",
    "    \n",
    "    for ax, (_, row) in zip(axes_flat, samples.iterrows()):\n",
    "        img_path = find_image_path(row['image_path'])\n",
    "        if img_path and Path(img_path).exists():\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_arr = np.array(img)\n",
    "            ax.imshow(img_arr)\n",
    "            \n",
    "            # Draw bounding boxes\n",
    "            if row['annotation_path']:\n",
    "                boxes = parse_yolo_annotation(row['annotation_path'])\n",
    "                draw_yolo_boxes(ax, img_arr, boxes, CLASS_NAMES)\n",
    "            \n",
    "            ax.set_title(f'{row[\"num_objects\"]} objects', fontsize=9)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Not found', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for ax in list(axes_flat)[len(samples):]:\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{config[\"name\"]} — Sample Images with Bounding Boxes', \n",
    "                 fontsize=14, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "elif config['format'] == 'folder':\n",
    "    # Show samples per class\n",
    "    SAMPLES_PER_CLASS = 5\n",
    "    num_classes_display = min(NUM_CLASSES, 15)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_classes_display, SAMPLES_PER_CLASS,\n",
    "                              figsize=(3 * SAMPLES_PER_CLASS, 3 * num_classes_display))\n",
    "    \n",
    "    for row_idx, (class_idx, class_name) in enumerate(sorted(CLASS_NAMES.items())[:num_classes_display]):\n",
    "        class_df = df[df['label'] == class_idx]\n",
    "        samples = class_df.sample(n=min(SAMPLES_PER_CLASS, len(class_df)), random_state=42)\n",
    "        \n",
    "        for col_idx, (_, sample) in enumerate(samples.iterrows()):\n",
    "            ax = axes[row_idx][col_idx] if num_classes_display > 1 else axes[col_idx]\n",
    "            img_path = find_image_path(sample['image_path'])\n",
    "            \n",
    "            if img_path and Path(img_path).exists():\n",
    "                img = Image.open(img_path)\n",
    "                ax.imshow(img)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'Not found', ha='center', va='center', transform=ax.transAxes)\n",
    "            \n",
    "            ax.axis('off')\n",
    "            if col_idx == 0:\n",
    "                ax.set_title(f'{class_name}\\n(n={len(class_df)})', \n",
    "                            fontsize=10, fontweight='bold', loc='left')\n",
    "    \n",
    "    plt.suptitle(f'{config[\"name\"]} — Samples per Class', fontsize=14, fontweight='bold', y=1.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755f6ca",
   "metadata": {},
   "source": [
    "### Visual Observations Checklist\n",
    "\n",
    "After looking at the samples above, answer these questions (edit this cell with your notes):\n",
    "\n",
    "- [ ] **Can you distinguish classes visually?** For YOLO: are crop/weed boxes accurate?\n",
    "- [ ] **Image quality:** Are images generally sharp and well-lit?\n",
    "- [ ] **Background variation:** Do images have diverse backgrounds?\n",
    "- [ ] **Annotation quality:** For YOLO: do boxes tightly fit the objects?\n",
    "- [ ] **Scale variation:** Are objects shown at different sizes?\n",
    "- [ ] **Edge cases:** Any images that seem mislabeled or ambiguous?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4059ce",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Image Properties Analysis\n",
    "\n",
    "**Why this matters for training:**\n",
    "- **Dimensions** — Models expect fixed input size. If images vary, we need resizing/cropping.\n",
    "- **Color channels** — RGB (3 channels) vs grayscale (1 channel) affects model architecture.\n",
    "- **File size** — Affects loading speed. Large files = slower data pipeline.\n",
    "- **Aspect ratio** — Square images are easier to work with. Non-square requires padding or cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413cead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a subset for analysis\n",
    "ANALYSIS_SAMPLE_SIZE = 500\n",
    "sample_df = df.sample(n=min(ANALYSIS_SAMPLE_SIZE, len(df)), random_state=42)\n",
    "\n",
    "widths, heights, channels, file_sizes = [], [], [], []\n",
    "errors = []\n",
    "\n",
    "for _, row in sample_df.iterrows():\n",
    "    img_path = find_image_path(row[FNAME_COL])\n",
    "    if img_path and Path(img_path).exists():\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            c = len(img.getbands())\n",
    "            widths.append(w)\n",
    "            heights.append(h)\n",
    "            channels.append(c)\n",
    "            file_sizes.append(Path(img_path).stat().st_size / 1024)  # KB\n",
    "        except Exception as e:\n",
    "            errors.append((row[FNAME_COL], str(e)))\n",
    "    else:\n",
    "        errors.append((row[FNAME_COL], 'File not found'))\n",
    "\n",
    "print(f'Analyzed {len(widths)} images (sampled from {len(df)})\\n')\n",
    "print(f'Width  — min: {min(widths)}, max: {max(widths)}, unique: {len(set(widths))}')\n",
    "print(f'Height — min: {min(heights)}, max: {max(heights)}, unique: {len(set(heights))}')\n",
    "print(f'Channels — unique: {set(channels)} ({\"RGB\" if 3 in channels else \"Grayscale\"})')\n",
    "print(f'File size — min: {min(file_sizes):.1f} KB, max: {max(file_sizes):.1f} KB, mean: {np.mean(file_sizes):.1f} KB')\n",
    "print(f'Errors: {len(errors)}')\n",
    "\n",
    "if errors:\n",
    "    print(f'\\nFirst 5 errors:')\n",
    "    for fname, err in errors[:5]:\n",
    "        print(f'  {fname}: {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188aa06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "axes[0].hist(widths, bins=30, color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Image Width Distribution')\n",
    "axes[0].set_xlabel('Width (px)')\n",
    "\n",
    "axes[1].hist(heights, bins=30, color='coral', edgecolor='white')\n",
    "axes[1].set_title('Image Height Distribution')\n",
    "axes[1].set_xlabel('Height (px)')\n",
    "\n",
    "axes[2].hist(file_sizes, bins=30, color='mediumseagreen', edgecolor='white')\n",
    "axes[2].set_title('File Size Distribution')\n",
    "axes[2].set_xlabel('Size (KB)')\n",
    "\n",
    "plt.suptitle(f'{config[\"name\"]} — Image Properties (sample of {len(widths)})', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4d2cd",
   "metadata": {},
   "source": [
    "### Training Implications\n",
    "\n",
    "**If all images are the same size:**\n",
    "- No resizing needed — can feed directly to the model\n",
    "- Consistent size simplifies the data pipeline\n",
    "\n",
    "**If images vary in size (common for smartphone datasets):**\n",
    "- Need a `Resize()` transform in the data pipeline\n",
    "- For classification: resize to 224x224 or 384x384 (EfficientNet defaults)\n",
    "- For detection: resize to 640x640 (YOLO default)\n",
    "- Choose: resize (distorts aspect ratio) vs center-crop (loses edges) vs pad (adds borders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cef06",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Per-Class Image Statistics\n",
    "\n",
    "**Why?** Different classes might have different visual characteristics (brightness, color).  \n",
    "This tells us whether color-based augmentation (brightness, contrast jitter) could help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b967199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean pixel values per class\n",
    "class_stats = {}\n",
    "STATS_SAMPLE = 50  # images per class\n",
    "\n",
    "if config['format'] == 'folder':\n",
    "    # Per-class color stats (each image = one class)\n",
    "    for class_idx, class_name in sorted(CLASS_NAMES.items()):\n",
    "        class_df = df[df['label'] == class_idx].sample(\n",
    "            n=min(STATS_SAMPLE, len(df[df['label'] == class_idx])), random_state=42\n",
    "        )\n",
    "        \n",
    "        pixel_means = []\n",
    "        pixel_stds = []\n",
    "        \n",
    "        for _, row in class_df.iterrows():\n",
    "            img_path = find_image_path(row[FNAME_COL])\n",
    "            if img_path and Path(img_path).exists():\n",
    "                img = np.array(Image.open(img_path).convert('RGB')) / 255.0\n",
    "                pixel_means.append(img.mean(axis=(0, 1)))\n",
    "                pixel_stds.append(img.std(axis=(0, 1)))\n",
    "        \n",
    "        if pixel_means:\n",
    "            class_stats[class_name] = {\n",
    "                'mean_rgb': np.mean(pixel_means, axis=0),\n",
    "                'std_rgb': np.mean(pixel_stds, axis=0),\n",
    "                'brightness': np.mean([m.mean() for m in pixel_means])\n",
    "            }\n",
    "\n",
    "elif config['format'] == 'yolo':\n",
    "    # For YOLO: overall color stats (images contain multiple classes)\n",
    "    sample = df.sample(n=min(200, len(df)), random_state=42)\n",
    "    pixel_means = []\n",
    "    pixel_stds = []\n",
    "    \n",
    "    for _, row in sample.iterrows():\n",
    "        img_path = find_image_path(row[FNAME_COL])\n",
    "        if img_path and Path(img_path).exists():\n",
    "            img = np.array(Image.open(img_path).convert('RGB')) / 255.0\n",
    "            pixel_means.append(img.mean(axis=(0, 1)))\n",
    "            pixel_stds.append(img.std(axis=(0, 1)))\n",
    "    \n",
    "    if pixel_means:\n",
    "        class_stats['All Images'] = {\n",
    "            'mean_rgb': np.mean(pixel_means, axis=0),\n",
    "            'std_rgb': np.mean(pixel_stds, axis=0),\n",
    "            'brightness': np.mean([m.mean() for m in pixel_means])\n",
    "        }\n",
    "\n",
    "# Display\n",
    "if class_stats:\n",
    "    stats_df = pd.DataFrame({\n",
    "        name: {\n",
    "            'R_mean': f\"{s['mean_rgb'][0]:.3f}\",\n",
    "            'G_mean': f\"{s['mean_rgb'][1]:.3f}\",\n",
    "            'B_mean': f\"{s['mean_rgb'][2]:.3f}\",\n",
    "            'Brightness': f\"{s['brightness']:.3f}\"\n",
    "        }\n",
    "        for name, s in class_stats.items()\n",
    "    }).T\n",
    "    \n",
    "    print('Per-class mean pixel values (0-1 scale):')\n",
    "    display(stats_df)\n",
    "else:\n",
    "    print('No color statistics computed (no images found).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dadd9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual: brightness/color comparison\n",
    "if class_stats and len(class_stats) > 1:\n",
    "    names = list(class_stats.keys())\n",
    "    brightness = [class_stats[n]['brightness'] for n in names]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, max(5, len(names) * 0.5)))\n",
    "    \n",
    "    bars = axes[0].barh(names, brightness, color='goldenrod')\n",
    "    axes[0].set_xlabel('Mean Brightness (0-1)')\n",
    "    axes[0].set_title('Average Brightness per Class')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    \n",
    "    x = np.arange(len(names))\n",
    "    width = 0.25\n",
    "    r_vals = [class_stats[n]['mean_rgb'][0] for n in names]\n",
    "    g_vals = [class_stats[n]['mean_rgb'][1] for n in names]\n",
    "    b_vals = [class_stats[n]['mean_rgb'][2] for n in names]\n",
    "    \n",
    "    axes[1].barh(x - width, r_vals, width, color='red', alpha=0.7, label='Red')\n",
    "    axes[1].barh(x, g_vals, width, color='green', alpha=0.7, label='Green')\n",
    "    axes[1].barh(x + width, b_vals, width, color='blue', alpha=0.7, label='Blue')\n",
    "    axes[1].set_yticks(x)\n",
    "    axes[1].set_yticklabels(names)\n",
    "    axes[1].set_xlabel('Mean Channel Value (0-1)')\n",
    "    axes[1].set_title('Mean RGB Channels per Class')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.suptitle(f'{config[\"name\"]} — Per-Class Color Statistics', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif class_stats:\n",
    "    # Single entry (YOLO) — just show RGB bars\n",
    "    name = list(class_stats.keys())[0]\n",
    "    s = class_stats[name]\n",
    "    print(f'{name}:')\n",
    "    print(f'  Mean RGB: R={s[\"mean_rgb\"][0]:.3f}, G={s[\"mean_rgb\"][1]:.3f}, B={s[\"mean_rgb\"][2]:.3f}')\n",
    "    print(f'  Brightness: {s[\"brightness\"]:.3f}')\n",
    "    print(f'\\nNote: YOLO images contain multiple classes per image,')\n",
    "    print(f'so per-class color stats are not meaningful. Overall stats shown instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8300ac30",
   "metadata": {},
   "source": [
    "### What This Tells Us\n",
    "\n",
    "- **High green channel** = images are outdoor vegetation scenes (expected for weed/crop data)\n",
    "- **Similar brightness across classes** = the model can't rely on brightness alone — it must learn texture/shape features\n",
    "- **If one class is much darker/brighter** = brightness augmentation will help the model generalize\n",
    "\n",
    "**Normalization:** When training, we'll normalize images to ImageNet statistics `(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])` because we're using ImageNet-pretrained backbones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901baf3",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Data Quality Check\n",
    "\n",
    "Quick checks for potential issues before training:\n",
    "- For YOLO: image/annotation file pairing, coordinate validation\n",
    "- For folders: duplicate images, missing files, corrupted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bd625",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['format'] == 'yolo':\n",
    "    # Check image-annotation pairing\n",
    "    with_ann = df['has_annotation'].sum()\n",
    "    without_ann = (~df['has_annotation']).sum()\n",
    "    print(f'Images WITH annotation: {with_ann}')\n",
    "    print(f'Images WITHOUT annotation: {without_ann}')\n",
    "    \n",
    "    if without_ann > 0:\n",
    "        print(f'\\nImages missing annotations (first 10):')\n",
    "        missing_ann = df[~df['has_annotation']].head(10)\n",
    "        for _, row in missing_ann.iterrows():\n",
    "            print(f'  {Path(row[\"image_path\"]).name}')\n",
    "    \n",
    "    # Validate YOLO coordinates are within [0, 1]\n",
    "    invalid_count = 0\n",
    "    checked = 0\n",
    "    for _, row in df[df['has_annotation']].iterrows():\n",
    "        boxes = parse_yolo_annotation(row['annotation_path'])\n",
    "        for box in boxes:\n",
    "            checked += 1\n",
    "            if not (0 <= box['cx'] <= 1 and 0 <= box['cy'] <= 1 and\n",
    "                    0 <= box['w'] <= 1 and 0 <= box['h'] <= 1):\n",
    "                invalid_count += 1\n",
    "    \n",
    "    print(f'\\nAnnotation coordinate validation:')\n",
    "    print(f'  Checked: {checked} boxes')\n",
    "    print(f'  Invalid (outside 0-1): {invalid_count}')\n",
    "    if invalid_count == 0:\n",
    "        print(f'  All coordinates valid.')\n",
    "    else:\n",
    "        print(f'  {invalid_count} boxes have out-of-range coordinates — may cause issues.')\n",
    "\n",
    "elif config['format'] == 'folder':\n",
    "    # Check for duplicates\n",
    "    filenames = df['image_path'].apply(lambda p: Path(p).name)\n",
    "    dupes = filenames[filenames.duplicated(keep=False)]\n",
    "    print(f'Duplicate filenames: {dupes.nunique()} unique names appear multiple times')\n",
    "    if len(dupes) > 0:\n",
    "        print(f'Total duplicate entries: {len(dupes)}')\n",
    "    \n",
    "    # Check for missing/corrupted files (sample)\n",
    "    check_sample = df.sample(n=min(200, len(df)), random_state=42)\n",
    "    missing = []\n",
    "    corrupted = []\n",
    "    for _, row in check_sample.iterrows():\n",
    "        path = find_image_path(row['image_path'])\n",
    "        if path is None or not Path(path).exists():\n",
    "            missing.append(row['image_path'])\n",
    "        else:\n",
    "            try:\n",
    "                Image.open(path).verify()\n",
    "            except Exception:\n",
    "                corrupted.append(row['image_path'])\n",
    "    \n",
    "    print(f'\\nMissing files (in sample of {len(check_sample)}): {len(missing)}')\n",
    "    print(f'Corrupted files (in sample of {len(check_sample)}): {len(corrupted)}')\n",
    "    if missing:\n",
    "        print(f'Missing rate estimate: {len(missing)/len(check_sample)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204295ad",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Dataset Summary for Training\n",
    "\n",
    "Everything we need to know to configure the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1874e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary dict that we can reference in later notebooks\n",
    "summary = {\n",
    "    'dataset': config['name'],\n",
    "    'active_key': ACTIVE_DATASET,\n",
    "    'task': config['task'],\n",
    "    'format': config['format'],\n",
    "    'total_images': len(df),\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'image_size': f'{min(widths)}x{min(heights)}' if len(set(widths)) == 1 and len(set(heights)) == 1 else 'varies',\n",
    "    'channels': 3,\n",
    "    'has_segmentation_masks': False,\n",
    "    'platform': PLATFORM,\n",
    "    'data_root': str(DATA_ROOT),\n",
    "}\n",
    "\n",
    "if config['format'] == 'yolo':\n",
    "    summary['total_objects'] = int(df['num_objects'].sum())\n",
    "    summary['mean_objects_per_image'] = round(df['num_objects'].mean(), 1)\n",
    "\n",
    "print(f'=== {config[\"name\"]} — Dataset Summary ===')\n",
    "print(json.dumps({k: v for k, v in summary.items() if k != 'class_names'}, indent=2))\n",
    "\n",
    "print(f'\\n=== Training Configuration Recommendations ===')\n",
    "if config['format'] == 'yolo':\n",
    "    print(f'Task: Object detection')\n",
    "    print(f'Input size: 640x640 (YOLO default) — resize needed if images differ')\n",
    "    print(f'Format: Already in YOLO format — ready for YOLOv5/v8 training')\n",
    "    print(f'Note: This is a 2-class detector. For species-level classification, use Bangladesh Rice Weed.')\n",
    "elif config['format'] == 'folder':\n",
    "    print(f'Task: Classification')\n",
    "    print(f'Input size: 224x224 (EfficientNet default) — resize transform required')\n",
    "    print(f'Normalize to: ImageNet stats (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])')\n",
    "    print(f'Augmentation: RandomHorizontalFlip, RandomVerticalFlip, ColorJitter, RandomRotation')\n",
    "\n",
    "if config['format'] == 'folder':\n",
    "    imbalance = class_counts.max() / class_counts.min()\n",
    "    if imbalance > 3:\n",
    "        print(f'Class imbalance ({imbalance:.1f}x) — use weighted CrossEntropyLoss or oversample minority')\n",
    "    else:\n",
    "        print(f'Class balance OK ({imbalance:.1f}x) — standard CrossEntropyLoss should work')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff542b3",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Segmentation Dataset Survey\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Neither the Crop & Weed Detection nor Bangladesh Rice Weed datasets include **pixel-level masks**. For our segmentation deliverable (D2: DeepLabV3+), we need a dataset where each image has a corresponding **mask** that marks exactly which pixels are \"weed\" and which are \"background\".\n",
    "\n",
    "### What is a Segmentation Mask?\n",
    "\n",
    "```\n",
    "Original Image          Segmentation Mask\n",
    "+------------------+    +------------------+\n",
    "|  crop crop  soil |    |  1  1  0  0  0   |   0 = background\n",
    "|  crop  soil weed |    |  1  0  0  2  2   |   1 = crop\n",
    "| soil  weed weed  |    |  0  2  2  2  2   |   2 = weed\n",
    "+------------------+    +------------------+\n",
    "```\n",
    "\n",
    "Each pixel in the mask has a class label. The model learns to predict this mask from the image.\n",
    "\n",
    "### Recommended: RiceSEG\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| **Images** | 3,078 |\n",
    "| **Resolution** | 512x512 pixels |\n",
    "| **Classes** | 6: Background, Green vegetation, Senescent vegetation, Panicle, **Weeds**, **Duckweed** |\n",
    "| **Source** | 5 countries: China, Japan, India, Philippines, Tanzania |\n",
    "| **Format** | Image + pixel-level mask pairs |\n",
    "| **Relevance** | Rice field weeds from tropical/subtropical climates |\n",
    "| **Available at** | HuggingFace (must upload to Kaggle as private dataset) |\n",
    "\n",
    "### Why RiceSEG?\n",
    "\n",
    "1. **Rice-field specific** — not generic agriculture, but actual rice paddy environments\n",
    "2. **Weed classes** — explicitly labels weeds and duckweed at the pixel level\n",
    "3. **Multi-country** — Philippines subset is closest to Indonesian conditions\n",
    "4. **Proper size** — 3,078 images is enough for a POC segmentation model\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- Weed pixels are **sparse** (~1.6% of total pixels) due to herbicide use at collection sites\n",
    "- This makes training harder — will need focal loss or heavy class weighting\n",
    "- The **Philippines subset** is most relevant (closest tropical climate to Indonesia)\n",
    "- See **notebook 02** for detailed RiceSEG exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d3e34",
   "metadata": {},
   "source": [
    "### Getting RiceSEG for Kaggle\n",
    "\n",
    "1. **Download** from HuggingFace (search: \"RiceSEG\")\n",
    "2. **Create a private Kaggle Dataset:**\n",
    "   - Go to kaggle.com > Datasets > New Dataset\n",
    "   - Upload the extracted RiceSEG folder\n",
    "   - Name it `riceseg`\n",
    "   - Set visibility to **Private**\n",
    "3. **Attach** to notebook 02 via \"Add Data\" sidebar\n",
    "\n",
    "This is a one-time setup. Once uploaded, you can attach RiceSEG to any notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651f89a",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Dataset explored:** See the summary above for class counts, image properties, and quality\n",
    "2. **No segmentation masks** in this dataset — need RiceSEG for D2 (explored in notebook 02)\n",
    "3. **Class imbalance** — check the distribution plots; may need weighted loss\n",
    "4. **Image quality** — review your visual observations from section 5\n",
    "5. **Format-specific notes:** YOLO annotations validated (if applicable)\n",
    "\n",
    "### What's Next\n",
    "\n",
    "| Next Notebook | What It Does | Dataset Needed |\n",
    "|---------------|-------------|----------------|\n",
    "| `02-segmentation-exploration.ipynb` | Explore RiceSEG masks and class distribution | RiceSEG (from HuggingFace) |\n",
    "| `04-classification-baseline.ipynb` | Train EfficientNetV2-S classifier | Crop & Weed (YOLO) or Bangladesh Rice Weed |\n",
    "\n",
    "**Recommended path:**\n",
    "- **PATH A:** Notebook 04 (classification) — start with Crop & Weed Detection (already on Kaggle), upgrade to Bangladesh for better species-level accuracy\n",
    "- **PATH B:** Notebook 02 (segmentation exploration) — explore RiceSEG before training DeepLabV3+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
