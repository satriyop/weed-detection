{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# 03 \u2014 Segmentation Training: DeepLabV3+ on RiceSEG\n\n**Purpose:** Train a semantic segmentation model to pixel-classify rice field weeds.\n**Runtime:** GPU required (Kaggle P100 recommended). Expect ~30-45 min total training.\n**Platform:** Works on Kaggle, Colab, and local (with GPU).\n**Prerequisite:** Run notebook 02 first to understand the dataset and class distribution.\n\n## What This Notebook Does\n\n1. **Setup** \u2014 Platform detection, dependency install, W&B logging init\n2. **Data pipeline** \u2014 Image-mask pairs, train/val split, augmentation transforms\n3. **Model** \u2014 DeepLabV3+ with ResNet-50 backbone (ImageNet pretrained)\n4. **Training Phase 1** \u2014 Frozen encoder, train decoder only (5 epochs)\n5. **Training Phase 2** \u2014 Unfreeze encoder, fine-tune end-to-end (15 epochs)\n6. **Evaluation** \u2014 Per-class IoU, mIoU, visual prediction overlays\n7. **Export** \u2014 Save best model checkpoint\n\n### Key Challenge: Weed Pixel Sparsity\n\nFrom notebook 02: weed pixels are only ~1.6% of total pixels. A naive model achieves >98%\naccuracy by predicting \"not weed\" everywhere. We use **focal loss** with class weights\nto force the model to learn weed features.\n\n| Parameter | Value |\n|-----------|-------|\n| **Architecture** | DeepLabV3+ (ResNet-50 encoder) |\n| **Input** | 384x384 crop (train) / 512x512 (val) |\n| **Classes** | 6: Background, Green veg, Senescent veg, Panicle, Weeds, Duckweed |\n| **Loss** | Focal loss (gamma=2) with inverse-frequency class weights |\n| **Expected mIoU** | 40-55% overall, 10-30% weed IoU |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "---\n## 1. Platform Detection & Setup\n\nSame pattern as notebooks 01 and 02 \u2014 detect Kaggle vs Colab vs local."
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "metadata": {},
   "source": "import os\nimport sys\n\n# --- Platform Detection ---\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\ntry:\n    import google.colab\n    IS_COLAB = True\nexcept ImportError:\n    IS_COLAB = False\n\nIS_LOCAL = not IS_KAGGLE and not IS_COLAB\n\nPLATFORM = 'kaggle' if IS_KAGGLE else ('colab' if IS_COLAB else 'local')\nprint(f'Platform detected: {PLATFORM}')\nprint(f'Python version: {sys.version}')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "### Install Dependencies\n\nTraining requires PyTorch, `segmentation-models-pytorch` (DeepLabV3+ implementation),\n`albumentations` (augmentation), and `wandb` (experiment tracking).\n\n**On Kaggle/Colab:** PyTorch and torchvision are pre-installed."
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "metadata": {},
   "source": "import subprocess\n\npackages = ['segmentation-models-pytorch', 'albumentations', 'wandb', 'scikit-learn']\n\nfor pkg in packages:\n    mod = pkg.replace('-', '_')\n    try:\n        __import__(mod)\n    except ImportError:\n        print(f'Installing {pkg}...')\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n\nprint('All dependencies ready.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {},
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom collections import Counter\nimport json\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport segmentation_models_pytorch as smp\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\n\nimport wandb\n\n# Consistent plot style\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['font.size'] = 11\n\n# Device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\nif DEVICE.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\nelse:\n    print('WARNING: No GPU detected. Training will be extremely slow.')\n\nprint('\\nImports ready.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": "### W&B Experiment Tracking (Optional)\n\nWeights & Biases logs training metrics, plots, and model artifacts to the cloud.\nSet `USE_WANDB = False` to disable \u2014 metrics still print to stdout."
  },
  {
   "cell_type": "code",
   "id": "cell-7",
   "metadata": {},
   "source": "USE_WANDB = True  # Set to False to skip W&B logging\n\nif USE_WANDB:\n    try:\n        wandb.login()\n        print('W&B login successful.')\n    except Exception as e:\n        print(f'W&B login failed: {e}')\n        print('Continuing without W&B.')\n        USE_WANDB = False\nelse:\n    print('W&B disabled.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "---\n## 2. Load RiceSEG Dataset\n\n### Class Definitions\n\n| Class ID | Name | Training Priority |\n|----------|------|-------------------|\n| 0 | Background | Low \u2014 dominant class |\n| 1 | Green vegetation | Medium |\n| 2 | Senescent vegetation | Medium |\n| 3 | Panicle | Medium |\n| 4 | **Weeds** | **High** \u2014 primary target |\n| 5 | **Duckweed** | **High** \u2014 secondary target |"
  },
  {
   "cell_type": "code",
   "id": "cell-9",
   "metadata": {},
   "source": "# --- RiceSEG class definitions ---\nRICESEG_CLASSES = {\n    0: 'Background',\n    1: 'Green vegetation',\n    2: 'Senescent vegetation',\n    3: 'Panicle',\n    4: 'Weeds',\n    5: 'Duckweed',\n}\nNUM_CLASSES = len(RICESEG_CLASSES)\n\n# Colors for visualization (RGB)\nCLASS_COLORS = {\n    0: (0, 0, 0),          # Background \u2014 black\n    1: (0, 200, 0),        # Green vegetation\n    2: (200, 200, 0),      # Senescent vegetation\n    3: (255, 165, 0),      # Panicle \u2014 orange\n    4: (255, 0, 0),        # Weeds \u2014 red\n    5: (0, 100, 255),      # Duckweed \u2014 blue\n}\n\n# --- Set data path ---\nif IS_KAGGLE:\n    DATA_ROOT = Path('/kaggle/input/riceseg')\nelif IS_COLAB:\n    DATA_ROOT = Path('/content/riceseg')\nelse:\n    DATA_ROOT = Path('./data/riceseg')\n\nprint(f'Data root: {DATA_ROOT}')\nprint(f'Exists: {DATA_ROOT.exists()}')\nprint(f'Classes ({NUM_CLASSES}):')\nfor k, v in RICESEG_CLASSES.items():\n    print(f'  {k}: {v}')\n\nif not DATA_ROOT.exists():\n    print()\n    print('=' * 60)\n    print('RICESEG NOT FOUND')\n    print('=' * 60)\n    print('See notebook 02 for detailed setup instructions.')\n    print(f'Expected path: {DATA_ROOT}')\n    print()\n    print('Quick setup:')\n    print('  1. Download RiceSEG from HuggingFace')\n    print('  2. Upload as private Kaggle Dataset named \"riceseg\"')\n    print('  3. Attach to this notebook via \"Add Data\"')\n    print('=' * 60)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "### Build Image-Mask Pairs\n\nSame pairing logic as notebook 02 \u2014 match images to masks by filename across directories.\nTries multiple directory structures: `images/` + `masks/`, country-based subdirs, etc."
  },
  {
   "cell_type": "code",
   "id": "cell-11",
   "metadata": {},
   "source": "def find_image_mask_pairs(data_root):\n    \"\"\"Find image-mask pairs in the RiceSEG dataset.\n\n    Tries multiple common directory structures to match images to masks.\n    Returns: list of dicts with image_path, mask_path, country.\n    \"\"\"\n    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n    mask_dir_names = {'masks', 'mask', 'labels', 'label', 'annotations', 'annotation', 'gt', 'groundtruth'}\n    image_dir_names = {'images', 'image', 'img', 'rgb', 'input'}\n\n    pairs = []\n\n    # Strategy 1: Top-level images/ + masks/\n    for img_dir_name in image_dir_names:\n        img_dir = data_root / img_dir_name\n        if not img_dir.exists():\n            continue\n        for mask_dir_name in mask_dir_names:\n            mask_dir = data_root / mask_dir_name\n            if not mask_dir.exists():\n                continue\n            for img_file in sorted(img_dir.rglob('*')):\n                if img_file.suffix.lower() not in image_extensions:\n                    continue\n                for ext in image_extensions:\n                    mask_candidate = mask_dir / (img_file.stem + ext)\n                    if mask_candidate.exists():\n                        pairs.append({\n                            'image_path': str(img_file),\n                            'mask_path': str(mask_candidate),\n                            'country': 'unknown',\n                        })\n                        break\n\n    if pairs:\n        return pairs\n\n    # Strategy 2: country/images/ + country/masks/\n    for country_dir in sorted(data_root.iterdir()):\n        if not country_dir.is_dir():\n            continue\n        for img_dir_name in image_dir_names:\n            img_dir = country_dir / img_dir_name\n            if not img_dir.exists():\n                continue\n            for mask_dir_name in mask_dir_names:\n                mask_dir = country_dir / mask_dir_name\n                if not mask_dir.exists():\n                    continue\n                for img_file in sorted(img_dir.rglob('*')):\n                    if img_file.suffix.lower() not in image_extensions:\n                        continue\n                    for ext in image_extensions:\n                        mask_candidate = mask_dir / (img_file.stem + ext)\n                        if mask_candidate.exists():\n                            pairs.append({\n                                'image_path': str(img_file),\n                                'mask_path': str(mask_candidate),\n                                'country': country_dir.name,\n                            })\n                            break\n\n    if pairs:\n        return pairs\n\n    # Strategy 3: Find all images and match with nearby mask directories\n    all_images = sorted(f for f in data_root.rglob('*')\n                        if f.suffix.lower() in image_extensions)\n\n    by_dir = {}\n    for img in all_images:\n        by_dir.setdefault(str(img.parent), []).append(img)\n\n    for dir_path, imgs in by_dir.items():\n        dir_p = Path(dir_path)\n        parent = dir_p.parent\n        if dir_p.name.lower() in mask_dir_names:\n            continue\n        for mask_dir_name in mask_dir_names:\n            mask_dir = parent / mask_dir_name\n            if mask_dir.exists():\n                for img_file in imgs:\n                    for ext in image_extensions:\n                        mask_candidate = mask_dir / (img_file.stem + ext)\n                        if mask_candidate.exists():\n                            rel = img_file.relative_to(data_root)\n                            country = rel.parts[0] if len(rel.parts) > 2 else 'unknown'\n                            pairs.append({\n                                'image_path': str(img_file),\n                                'mask_path': str(mask_candidate),\n                                'country': country,\n                            })\n                            break\n    return pairs\n\n\n# --- Build pairs ---\nif DATA_ROOT.exists():\n    pairs = find_image_mask_pairs(DATA_ROOT)\n    df = pd.DataFrame(pairs)\n\n    print(f'Found {len(df)} image-mask pairs')\n    if len(df) > 0:\n        print(f'\\nCountry distribution:')\n        print(df['country'].value_counts().to_string())\n    else:\n        print('No pairs found. Check directory structure.')\n        print('Remaining cells will show \"no data\" messages.')\nelse:\n    df = pd.DataFrame()\n    print('Data root not found. Cannot load pairs.')\n    print('Remaining cells will show \"no data\" messages.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "---\n## 3. Train / Validation Split\n\n80% train, 20% validation. Random split with fixed seed for reproducibility.\n\n> **Note:** A country-stratified split would ensure each split has proportional representation\n> from all countries. For this baseline we use a simple random split \u2014 the model should\n> generalize across countries anyway."
  },
  {
   "cell_type": "code",
   "id": "cell-13",
   "metadata": {},
   "source": "if len(df) > 0:\n    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n\n    print(f'Train: {len(train_df):,} pairs')\n    print(f'Val:   {len(val_df):,} pairs')\n\n    if 'country' in df.columns and df['country'].nunique() > 1:\n        print(f'\\nTrain countries: {train_df[\"country\"].value_counts().to_dict()}')\n        print(f'Val countries:   {val_df[\"country\"].value_counts().to_dict()}')\nelse:\n    train_df = val_df = None\n    print('No data available for splitting.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "---\n## 4. Data Pipeline\n\n### Augmentation Strategy\n\n| Transform | Train | Val | Why |\n|-----------|:-----:|:---:|-----|\n| RandomCrop 384x384 | Yes | \u2014 | Augmentation + memory savings |\n| HorizontalFlip | Yes | \u2014 | Rice fields look same mirrored |\n| VerticalFlip | Yes | \u2014 | Top-down views are rotation-invariant |\n| RandomRotate90 | Yes | \u2014 | Further rotation invariance |\n| ColorJitter | Yes | \u2014 | Handle varying lighting conditions |\n| Normalize (ImageNet) | Yes | Yes | Required for pretrained backbone |\n\n**Key:** Albumentations applies the SAME spatial transform to both image and mask.\nColor transforms only affect the image \u2014 the mask's integer class IDs stay untouched."
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {},
   "source": "CROP_SIZE = 384  # Random crop from native 512x512\n\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\ntrain_transform = A.Compose([\n    A.RandomCrop(CROP_SIZE, CROP_SIZE),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.RandomRotate90(p=0.5),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    # No crop \u2014 evaluate on full 512x512 for best metrics.\n    # DeepLabV3+ is fully convolutional and handles any input size.\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nprint(f'Train: RandomCrop({CROP_SIZE}) + flip + rotate + color jitter + normalize')\nprint(f'Val:   normalize only (full 512x512)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-16",
   "metadata": {},
   "source": "class RiceSEGDataset(Dataset):\n    \"\"\"PyTorch dataset for RiceSEG segmentation.\"\"\"\n\n    def __init__(self, dataframe, transform=None):\n        self.df = dataframe.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = np.array(Image.open(row['image_path']).convert('RGB'))\n        mask = np.array(Image.open(row['mask_path']))\n\n        # Ensure mask is 2D (H, W) with integer class IDs\n        if mask.ndim == 3:\n            mask = mask[:, :, 0]\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed['image']\n            mask = transformed['mask']\n\n        return image, mask.long()\n\n\nif train_df is not None:\n    train_dataset = RiceSEGDataset(train_df, transform=train_transform)\n    val_dataset = RiceSEGDataset(val_df, transform=val_transform)\n\n    img, msk = train_dataset[0]\n    print(f'Train dataset: {len(train_dataset)} samples')\n    print(f'Val dataset:   {len(val_dataset)} samples')\n    print(f'Image shape:   {img.shape} (C, H, W)')\n    print(f'Mask shape:    {msk.shape} (H, W)')\n    print(f'Mask dtype:    {msk.dtype}')\n    print(f'Mask classes:  {torch.unique(msk).tolist()}')\nelse:\n    print('No data to create datasets.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-17",
   "metadata": {},
   "source": "BATCH_SIZE = 8      # Training (384x384) \u2014 reduce to 4 if OOM\nVAL_BATCH_SIZE = 4  # Validation (512x512) \u2014 larger images need smaller batch\nNUM_WORKERS = 2     # Kaggle has 2 CPU cores\n\nif train_df is not None:\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        drop_last=True,\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=VAL_BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    print(f'Train: batch_size={BATCH_SIZE}, {len(train_loader)} batches')\n    print(f'Val:   batch_size={VAL_BATCH_SIZE}, {len(val_loader)} batches')\nelse:\n    print('No data for DataLoaders.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "### Preview Augmented Samples\n\nSanity check: do augmented images and masks still align correctly after spatial transforms?"
  },
  {
   "cell_type": "code",
   "id": "cell-19",
   "metadata": {},
   "source": "def mask_to_rgb(mask, class_colors):\n    \"\"\"Convert class ID mask to RGB image for visualization.\"\"\"\n    h, w = mask.shape\n    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n    for cls_id, color in class_colors.items():\n        rgb[mask == cls_id] = color\n    return rgb\n\n\ndef denormalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n    \"\"\"Reverse ImageNet normalization for display.\"\"\"\n    mean = torch.tensor(mean).view(3, 1, 1)\n    std = torch.tensor(std).view(3, 1, 1)\n    return (tensor * std + mean).clamp(0, 1)\n\n\nif train_df is not None:\n    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\n    for row in range(3):\n        img, msk = train_dataset[row]\n        img_np = denormalize(img).permute(1, 2, 0).numpy()\n        msk_np = msk.numpy()\n        msk_rgb = mask_to_rgb(msk_np, CLASS_COLORS)\n\n        # Overlay: blend image with colored mask\n        overlay = img_np.copy()\n        for cls_id in range(1, NUM_CLASSES):  # skip background\n            cls_mask = msk_np == cls_id\n            if cls_mask.any():\n                color = np.array(CLASS_COLORS[cls_id]) / 255.0\n                overlay[cls_mask] = overlay[cls_mask] * 0.5 + color * 0.5\n\n        axes[row, 0].imshow(img_np)\n        axes[row, 0].set_title('Image', fontsize=10)\n        axes[row, 1].imshow(msk_rgb)\n        axes[row, 1].set_title('Mask', fontsize=10)\n        axes[row, 2].imshow(overlay)\n        axes[row, 2].set_title('Overlay', fontsize=10)\n\n        # Per-sample class histogram\n        unique, counts = np.unique(msk_np, return_counts=True)\n        names = [RICESEG_CLASSES.get(u, '?') for u in unique]\n        colors = [np.array(CLASS_COLORS.get(u, (128, 128, 128))) / 255.0 for u in unique]\n        axes[row, 3].barh(names, counts, color=colors)\n        axes[row, 3].set_title('Pixel counts', fontsize=10)\n\n        for ax in axes[row, :3]:\n            ax.axis('off')\n\n    plt.suptitle('Augmented Training Samples (384x384 crops)', fontsize=13, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    # Legend\n    print('Class colors:')\n    for cls_id, name in RICESEG_CLASSES.items():\n        r, g, b = CLASS_COLORS[cls_id]\n        print(f'  {cls_id}: {name} -- RGB({r}, {g}, {b})')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "---\n## 5. Model: DeepLabV3+ with ResNet-50\n\n### Why DeepLabV3+?\n\nDeepLabV3+ adds an **encoder-decoder** structure to DeepLabV3:\n- **Encoder** (ResNet-50): Extracts multi-scale features using atrous (dilated) convolutions\n- **ASPP** (Atrous Spatial Pyramid Pooling): Captures context at multiple scales\n- **Decoder**: Recovers spatial detail by combining low-level encoder features with high-level ASPP output\n\nThe decoder is what distinguishes DeepLabV3+ from DeepLabV3 \u2014 it produces sharper boundaries,\nwhich matters for small, irregular weed patches.\n\nWe use `segmentation-models-pytorch` (smp) for a clean implementation.\n\n### Transfer Learning Strategy\n\n| Phase | Encoder | Decoder/Head | LR | Epochs |\n|-------|---------|-------------|------|--------|\n| 1 (frozen) | Frozen (ImageNet) | Training | 1e-3 | 5 |\n| 2 (fine-tune) | Unfrozen | Training | 1e-4 | 15 |"
  },
  {
   "cell_type": "code",
   "id": "cell-21",
   "metadata": {},
   "source": "def create_model(num_classes, encoder='resnet50', pretrained=True):\n    \"\"\"Create DeepLabV3+ with specified encoder.\"\"\"\n    model = smp.DeepLabV3Plus(\n        encoder_name=encoder,\n        encoder_weights='imagenet' if pretrained else None,\n        in_channels=3,\n        classes=num_classes,\n    )\n    return model\n\n\ndef freeze_encoder(model):\n    \"\"\"Freeze encoder \u2014 only decoder trains.\"\"\"\n    for param in model.encoder.parameters():\n        param.requires_grad = False\n\n\ndef unfreeze_all(model):\n    \"\"\"Unfreeze all layers for end-to-end fine-tuning.\"\"\"\n    for param in model.parameters():\n        param.requires_grad = True\n\n\ndef count_parameters(model):\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    return trainable, total\n\n\nmodel = create_model(NUM_CLASSES)\nmodel = model.to(DEVICE)\n\n# Start with frozen encoder (Phase 1)\nfreeze_encoder(model)\ntrainable, total = count_parameters(model)\n\nprint(f'Model: DeepLabV3+ (ResNet-50 encoder)')\nprint(f'Total parameters:     {total:>12,}')\nprint(f'Trainable parameters: {trainable:>12,} (decoder only)')\nprint(f'Frozen parameters:    {total - trainable:>12,} (encoder)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "### Loss Function: Focal Loss\n\nStandard cross-entropy treats all pixels equally, so the model learns to predict \"background\"\neverywhere (>98% accuracy for free). **Focal loss** down-weights easy examples and focuses\non hard ones:\n\n**FL(p_t) = -alpha_t (1 - p_t)^gamma log(p_t)**\n\n- **gamma:** Controls focus. gamma=0 is standard CE. gamma=2 strongly penalizes confident wrong predictions.\n- **alpha:** Per-class weights. Weed class gets high weight to compensate for sparsity."
  },
  {
   "cell_type": "code",
   "id": "cell-23",
   "metadata": {},
   "source": "# Compute class weights from training data (inverse frequency, normalized to mean=1)\nif train_df is not None:\n    print('Computing class pixel distribution from training set...')\n    class_pixel_counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n    total_pixels = 0\n\n    for i, (_, row) in enumerate(train_df.iterrows()):\n        try:\n            mask = np.array(Image.open(row['mask_path']))\n            if mask.ndim == 3:\n                mask = mask[:, :, 0]\n            for cls_id in range(NUM_CLASSES):\n                class_pixel_counts[cls_id] += np.sum(mask == cls_id)\n            total_pixels += mask.size\n        except Exception:\n            pass\n        if (i + 1) % 500 == 0:\n            print(f'  {i + 1}/{len(train_df)}...')\n\n    print(f'Done. Total pixels: {total_pixels:,}')\n\n    # Inverse frequency weights\n    freq = class_pixel_counts / total_pixels\n    raw_weights = 1.0 / np.where(freq > 0, freq, 1)\n    class_weights_np = raw_weights / raw_weights.mean()\n\n    CLASS_WEIGHTS = torch.tensor(class_weights_np, dtype=torch.float32)\n\n    print('\\nComputed class weights (inverse frequency, normalized):')\n    for i in range(NUM_CLASSES):\n        pct = freq[i] * 100\n        print(f'  {RICESEG_CLASSES[i]:25s}: {pct:6.2f}% pixels -> weight {CLASS_WEIGHTS[i]:.2f}')\nelse:\n    # Fallback weights (approximate, from notebook 02 analysis)\n    CLASS_WEIGHTS = torch.tensor([0.03, 0.08, 0.30, 0.50, 3.00, 2.00], dtype=torch.float32)\n    CLASS_WEIGHTS = CLASS_WEIGHTS / CLASS_WEIGHTS.mean()\n    print('Using fallback class weights (data not available).')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-24",
   "metadata": {},
   "source": "class FocalLoss(nn.Module):\n    \"\"\"Focal Loss for semantic segmentation with per-class weights.\n\n    Args:\n        alpha: Per-class weight tensor of shape (num_classes,).\n        gamma: Focusing parameter. 0 = standard CE. 2 = strong focus on hard examples.\n        ignore_index: Class index to ignore in loss computation.\n    \"\"\"\n\n    def __init__(self, alpha=None, gamma=2.0, ignore_index=-1):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n\n    def forward(self, logits, targets):\n        # logits: (B, C, H, W), targets: (B, H, W)\n        ce_loss = F.cross_entropy(\n            logits, targets,\n            reduction='none',\n            ignore_index=self.ignore_index,\n        )\n\n        # p_t = probability assigned to the correct class\n        pt = torch.exp(-ce_loss)\n\n        # Focal modulation: down-weight easy examples\n        focal_loss = (1 - pt) ** self.gamma * ce_loss\n\n        # Apply per-class weights\n        if self.alpha is not None:\n            # Clamp targets to valid range for indexing (ignore_index might be -1)\n            alpha_t = self.alpha.to(logits.device)[targets.clamp(0)]\n            focal_loss = alpha_t * focal_loss\n\n        return focal_loss.mean()\n\n\nGAMMA = 2.0\ncriterion = FocalLoss(alpha=CLASS_WEIGHTS, gamma=GAMMA)\n\nprint(f'Loss: Focal Loss (gamma={GAMMA})')\nprint(f'Class weights applied: yes ({NUM_CLASSES} classes)')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": "### Evaluation Metrics: IoU (Intersection over Union)\n\nFor segmentation, **pixel accuracy is misleading** (>98% by predicting background everywhere).\nWe use IoU instead:\n\n**IoU = Intersection / Union** for each class separately.\n\n- **mIoU:** Mean IoU across all classes \u2014 the standard segmentation benchmark metric.\n- **Weed IoU:** The single most important metric for our task.\n- IoU ranges from 0 (no overlap) to 1 (perfect overlap)."
  },
  {
   "cell_type": "code",
   "id": "cell-26",
   "metadata": {},
   "source": "def compute_epoch_iou(intersection_sum, union_sum, num_classes):\n    \"\"\"Compute per-class IoU from accumulated intersection/union counts.\"\"\"\n    ious = {}\n    for cls in range(num_classes):\n        if union_sum[cls] > 0:\n            ious[cls] = intersection_sum[cls] / union_sum[cls]\n        else:\n            ious[cls] = float('nan')\n\n    valid_ious = [v for v in ious.values() if not np.isnan(v)]\n    miou = np.mean(valid_ious) if valid_ious else 0.0\n\n    return ious, miou\n\n\ndef update_iou_accumulators(preds, targets, intersection_sum, union_sum, num_classes):\n    \"\"\"Update running intersection/union counts for IoU computation.\"\"\"\n    for cls in range(num_classes):\n        pred_cls = (preds == cls)\n        target_cls = (targets == cls)\n        intersection_sum[cls] += (pred_cls & target_cls).sum()\n        union_sum[cls] += (pred_cls | target_cls).sum()\n\n\nprint('IoU metric functions defined.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-27",
   "metadata": {},
   "source": "# --- Phase 1 optimizer (frozen encoder) ---\nLR_PHASE1 = 1e-3\nEPOCHS_PHASE1 = 5\n\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=LR_PHASE1,\n)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE1)\n\nprint(f'Phase 1 config:')\nprint(f'  Optimizer:     Adam')\nprint(f'  Learning rate: {LR_PHASE1}')\nprint(f'  Epochs:        {EPOCHS_PHASE1}')\nprint(f'  Scheduler:     CosineAnnealingLR')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": "---\n## 6. Training Loop\n\nTwo-phase transfer learning:\n\n| Phase | What trains | LR | Epochs | Goal |\n|-------|------------|------|--------|------|\n| 1 | Decoder only | 1e-3 | 5 | Learn to decode ImageNet features into segmentation masks |\n| 2 | Everything | 1e-4 | 15 | Adapt encoder features to rice field weed patterns |"
  },
  {
   "cell_type": "code",
   "id": "cell-29",
   "metadata": {},
   "source": "def train_one_epoch(model, loader, criterion, optimizer, device, num_classes):\n    \"\"\"Train for one epoch. Returns loss and mIoU.\"\"\"\n    model.train()\n    running_loss = 0.0\n    total_samples = 0\n    intersection_sum = np.zeros(num_classes)\n    union_sum = np.zeros(num_classes)\n\n    for images, masks in loader:\n        images = images.to(device, non_blocking=True)\n        masks = masks.to(device, non_blocking=True)\n\n        optimizer.zero_grad()\n        logits = model(images)  # (B, C, H, W)\n        loss = criterion(logits, masks)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        total_samples += images.size(0)\n\n        # Accumulate IoU\n        preds = logits.argmax(dim=1)  # (B, H, W)\n        update_iou_accumulators(\n            preds.cpu().numpy(), masks.cpu().numpy(),\n            intersection_sum, union_sum, num_classes,\n        )\n\n    avg_loss = running_loss / total_samples\n    _, miou = compute_epoch_iou(intersection_sum, union_sum, num_classes)\n\n    return avg_loss, miou\n\n\ndef validate(model, loader, criterion, device, num_classes):\n    \"\"\"Validate the model. Returns loss, mIoU, and per-class IoUs.\"\"\"\n    model.eval()\n    running_loss = 0.0\n    total_samples = 0\n    intersection_sum = np.zeros(num_classes)\n    union_sum = np.zeros(num_classes)\n\n    with torch.no_grad():\n        for images, masks in loader:\n            images = images.to(device, non_blocking=True)\n            masks = masks.to(device, non_blocking=True)\n\n            logits = model(images)\n            loss = criterion(logits, masks)\n\n            running_loss += loss.item() * images.size(0)\n            total_samples += images.size(0)\n\n            preds = logits.argmax(dim=1)\n            update_iou_accumulators(\n                preds.cpu().numpy(), masks.cpu().numpy(),\n                intersection_sum, union_sum, num_classes,\n            )\n\n    avg_loss = running_loss / total_samples\n    ious, miou = compute_epoch_iou(intersection_sum, union_sum, num_classes)\n\n    return avg_loss, miou, ious\n\n\nprint('Training functions defined.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": "### Phase 1: Frozen Encoder\n\nOnly the decoder trains. The encoder acts as a fixed feature extractor\nusing ImageNet-learned representations (edges, textures, shapes).\nThis is fast \u2014 few parameters to update."
  },
  {
   "cell_type": "code",
   "id": "cell-31",
   "metadata": {},
   "source": "# --- W&B run init ---\nif USE_WANDB:\n    wandb.init(\n        project='agri-weed-detection',\n        config={\n            'model': 'deeplabv3plus_resnet50',\n            'task': 'segmentation',\n            'dataset': 'riceseg',\n            'crop_size': CROP_SIZE,\n            'batch_size': BATCH_SIZE,\n            'lr_phase1': LR_PHASE1,\n            'epochs_phase1': EPOCHS_PHASE1,\n            'gamma': GAMMA,\n            'num_classes': NUM_CLASSES,\n            'train_size': len(train_df) if train_df is not None else 0,\n            'val_size': len(val_df) if val_df is not None else 0,\n            'platform': PLATFORM,\n        },\n        tags=['baseline', 'segmentation', 'riceseg'],\n    )\n    print(f'W&B run: {wandb.run.name}')\n    print(f'W&B URL: {wandb.run.get_url()}')\n\n# --- Checkpoint tracking ---\nSAVE_DIR = Path('/kaggle/working') if IS_KAGGLE else Path('./checkpoints')\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\nbest_miou = 0.0\nbest_model_path = SAVE_DIR / 'deeplabv3plus_resnet50_riceseg_v1.pt'\n\nhistory = {\n    'train_loss': [], 'val_loss': [],\n    'train_miou': [], 'val_miou': [],\n    'val_weed_iou': [], 'lr': [],\n}\n\nprint(f'\\n=== Phase 1: Frozen Encoder ===')\nprint(f'Training decoder only ({count_parameters(model)[0]:,} params)')\nprint(f'Checkpoint saves to: {best_model_path}\\n')\n\nif train_df is not None:\n    for epoch in range(EPOCHS_PHASE1):\n        t0 = time.time()\n\n        train_loss, train_miou = train_one_epoch(\n            model, train_loader, criterion, optimizer, DEVICE, NUM_CLASSES,\n        )\n        val_loss, val_miou, val_ious = validate(\n            model, val_loader, criterion, DEVICE, NUM_CLASSES,\n        )\n\n        scheduler.step()\n        current_lr = scheduler.get_last_lr()[0]\n        elapsed = time.time() - t0\n\n        weed_iou = val_ious.get(4, float('nan'))\n\n        # Track history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_miou'].append(train_miou)\n        history['val_miou'].append(val_miou)\n        history['val_weed_iou'].append(weed_iou)\n        history['lr'].append(current_lr)\n\n        # Save best model by mIoU\n        marker = ''\n        if val_miou > best_miou:\n            best_miou = val_miou\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'val_miou': val_miou,\n                'val_loss': val_loss,\n                'val_ious': val_ious,\n                'num_classes': NUM_CLASSES,\n                'class_names': RICESEG_CLASSES,\n                'crop_size': CROP_SIZE,\n            }, best_model_path)\n            marker = ' * (best)'\n\n        print(f'Epoch [{epoch+1:2d}/{EPOCHS_PHASE1}] '\n              f'loss={train_loss:.4f}/{val_loss:.4f} '\n              f'mIoU={train_miou:.4f}/{val_miou:.4f} '\n              f'weed_IoU={weed_iou:.4f} '\n              f'lr={current_lr:.6f} '\n              f'({elapsed:.0f}s){marker}')\n\n        if USE_WANDB:\n            wandb.log({\n                'epoch': epoch + 1, 'phase': 1,\n                'train_loss': train_loss, 'val_loss': val_loss,\n                'train_miou': train_miou, 'val_miou': val_miou,\n                'val_weed_iou': weed_iou, 'lr': current_lr,\n            })\n\n    print(f'\\nPhase 1 complete. Best mIoU: {best_miou:.4f}')\nelse:\n    print('No training data. Skipping Phase 1.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": "### Phase 2: Full Fine-Tuning\n\nUnfreeze the encoder and train the entire model with a lower learning rate (1e-4).\nThis lets the ResNet-50 backbone adapt its features from generic ImageNet patterns\nto rice-field-specific patterns (leaf textures, weed shapes, color profiles).\n\n**Why lower LR?** The encoder weights are already well-trained on ImageNet. Large updates\nwould destroy learned features (\"catastrophic forgetting\"). Small, careful updates let the\nbackbone specialize without losing its foundation."
  },
  {
   "cell_type": "code",
   "id": "cell-33",
   "metadata": {},
   "source": "if train_df is not None:\n    LR_PHASE2 = 1e-4\n    EPOCHS_PHASE2 = 15\n\n    # Unfreeze everything\n    unfreeze_all(model)\n    trainable, total = count_parameters(model)\n\n    # New optimizer for all parameters\n    optimizer = optim.Adam(model.parameters(), lr=LR_PHASE2)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE2)\n\n    print(f'=== Phase 2: Full Fine-Tuning ===')\n    print(f'All parameters unfrozen ({trainable:,} trainable)\\n')\n\n    for epoch in range(EPOCHS_PHASE2):\n        t0 = time.time()\n        global_epoch = EPOCHS_PHASE1 + epoch\n\n        train_loss, train_miou = train_one_epoch(\n            model, train_loader, criterion, optimizer, DEVICE, NUM_CLASSES,\n        )\n        val_loss, val_miou, val_ious = validate(\n            model, val_loader, criterion, DEVICE, NUM_CLASSES,\n        )\n\n        scheduler.step()\n        current_lr = scheduler.get_last_lr()[0]\n        elapsed = time.time() - t0\n\n        weed_iou = val_ious.get(4, float('nan'))\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_miou'].append(train_miou)\n        history['val_miou'].append(val_miou)\n        history['val_weed_iou'].append(weed_iou)\n        history['lr'].append(current_lr)\n\n        marker = ''\n        if val_miou > best_miou:\n            best_miou = val_miou\n            torch.save({\n                'epoch': global_epoch,\n                'model_state_dict': model.state_dict(),\n                'val_miou': val_miou,\n                'val_loss': val_loss,\n                'val_ious': val_ious,\n                'num_classes': NUM_CLASSES,\n                'class_names': RICESEG_CLASSES,\n                'crop_size': CROP_SIZE,\n            }, best_model_path)\n            marker = ' * (best)'\n\n        print(f'Epoch [{global_epoch+1:2d}/{EPOCHS_PHASE1 + EPOCHS_PHASE2}] '\n              f'loss={train_loss:.4f}/{val_loss:.4f} '\n              f'mIoU={train_miou:.4f}/{val_miou:.4f} '\n              f'weed_IoU={weed_iou:.4f} '\n              f'lr={current_lr:.6f} '\n              f'({elapsed:.0f}s){marker}')\n\n        if USE_WANDB:\n            wandb.log({\n                'epoch': global_epoch + 1, 'phase': 2,\n                'train_loss': train_loss, 'val_loss': val_loss,\n                'train_miou': train_miou, 'val_miou': val_miou,\n                'val_weed_iou': weed_iou, 'lr': current_lr,\n            })\n\n    print(f'\\nPhase 2 complete. Best mIoU: {best_miou:.4f}')\nelse:\n    print('No training data. Skipping Phase 2.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": "### Training History\n\nVisualize how loss, mIoU, and weed IoU evolved across both phases."
  },
  {
   "cell_type": "code",
   "id": "cell-35",
   "metadata": {},
   "source": "if len(history['train_loss']) > 0:\n    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n    epochs_range = range(1, len(history['train_loss']) + 1)\n    phase_boundary = EPOCHS_PHASE1\n\n    # Loss\n    axes[0, 0].plot(epochs_range, history['train_loss'], 'b-', label='Train')\n    axes[0, 0].plot(epochs_range, history['val_loss'], 'r-', label='Val')\n    axes[0, 0].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Focal Loss')\n    axes[0, 0].set_title('Loss')\n    axes[0, 0].legend()\n\n    # mIoU\n    axes[0, 1].plot(epochs_range, history['train_miou'], 'b-', label='Train')\n    axes[0, 1].plot(epochs_range, history['val_miou'], 'r-', label='Val')\n    axes[0, 1].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('mIoU')\n    axes[0, 1].set_title('Mean IoU (all classes)')\n    axes[0, 1].legend()\n\n    # Weed IoU\n    weed_ious_valid = [v if not np.isnan(v) else 0 for v in history['val_weed_iou']]\n    axes[1, 0].plot(epochs_range[:len(weed_ious_valid)],\n                    weed_ious_valid, 'r-o', markersize=4, label='Weed IoU')\n    axes[1, 0].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Weed IoU')\n    axes[1, 0].set_title('Weed Class IoU (the metric that matters)')\n    axes[1, 0].legend()\n\n    # Learning rate\n    axes[1, 1].plot(epochs_range, history['lr'], 'g-')\n    axes[1, 1].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Learning Rate')\n    axes[1, 1].set_title('Learning Rate Schedule')\n    axes[1, 1].legend()\n\n    plt.suptitle('Training History (Phase 1: frozen encoder -> Phase 2: full fine-tune)',\n                 fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\nelse:\n    print('No training history to plot.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": "---\n## 7. Evaluation\n\nLoad the best checkpoint and run a thorough evaluation:\n- Per-class IoU bar chart\n- Visual predictions (image, ground truth mask, predicted mask, error map)"
  },
  {
   "cell_type": "code",
   "id": "cell-37",
   "metadata": {},
   "source": "if train_df is not None and best_model_path.exists():\n    # Load best checkpoint\n    checkpoint = torch.load(best_model_path, map_location=DEVICE, weights_only=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f'Loaded best model from epoch {checkpoint[\"epoch\"] + 1}')\n    print(f'Best val mIoU: {checkpoint[\"val_miou\"]:.4f}')\n\n    # Run final validation\n    val_loss, val_miou, val_ious = validate(\n        model, val_loader, criterion, DEVICE, NUM_CLASSES,\n    )\n\n    print(f'\\n=== Final Validation Results ===')\n    print(f'Loss:       {val_loss:.4f}')\n    print(f'mIoU:       {val_miou:.4f} ({val_miou*100:.1f}%)')\n    print(f'\\nPer-class IoU:')\n    for cls_id in range(NUM_CLASSES):\n        iou = val_ious.get(cls_id, float('nan'))\n        bar_len = int(iou * 40) if not np.isnan(iou) else 0\n        bar_str = '#' * bar_len\n        print(f'  {RICESEG_CLASSES[cls_id]:25s}: {iou:.4f} ({iou*100:.1f}%) {bar_str}')\n\n    weed_iou = val_ious.get(4, float('nan'))\n    print(f'\\n>>> Weed IoU: {weed_iou:.4f} ({weed_iou*100:.1f}%)')\nelse:\n    print('No model to evaluate.')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-38",
   "metadata": {},
   "source": "if train_df is not None and best_model_path.exists():\n    class_names_list = [RICESEG_CLASSES[i] for i in range(NUM_CLASSES)]\n    iou_values = [val_ious.get(i, 0) for i in range(NUM_CLASSES)]\n    bar_colors_list = [np.array(CLASS_COLORS[i]) / 255.0 for i in range(NUM_CLASSES)]\n\n    fig, ax = plt.subplots(figsize=(12, 5))\n    bars = ax.barh(class_names_list, iou_values, color=bar_colors_list, edgecolor='gray')\n    ax.set_xlabel('IoU')\n    ax.set_title(f'Per-Class IoU | mIoU: {val_miou:.4f}')\n    ax.set_xlim(0, 1)\n    ax.axvline(x=val_miou, color='gray', linestyle='--', alpha=0.5, label=f'mIoU={val_miou:.3f}')\n\n    for bar, iou in zip(bars, iou_values):\n        label = f'{iou:.3f}' if not np.isnan(iou) else 'N/A'\n        ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height() / 2,\n                label, va='center', fontsize=10)\n\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n    if USE_WANDB:\n        wandb.log({'per_class_iou': wandb.Image(fig)})\n        for cls_id in range(NUM_CLASSES):\n            wandb.log({f'iou_{RICESEG_CLASSES[cls_id]}': val_ious.get(cls_id, 0)})",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": "### Visual Predictions\n\nCompare ground truth masks with model predictions. The error map highlights\nwhere the model disagrees with the ground truth:\n- **Correct pixels:** dark\n- **Incorrect pixels:** bright (white)\n\nPay attention to weed regions (red in ground truth) \u2014 does the model detect them?"
  },
  {
   "cell_type": "code",
   "id": "cell-40",
   "metadata": {},
   "source": "if train_df is not None and best_model_path.exists():\n    model.eval()\n\n    # Select validation samples \u2014 prefer ones with weed pixels\n    sample_indices = []\n    for i in range(len(val_df)):\n        mask = np.array(Image.open(val_df.iloc[i]['mask_path']))\n        if mask.ndim == 3:\n            mask = mask[:, :, 0]\n        if np.sum(mask == 4) > 0:  # Has weed pixels\n            sample_indices.append(i)\n        if len(sample_indices) >= 6:\n            break\n\n    # If not enough weed samples, add random ones\n    if len(sample_indices) < 6:\n        remaining = [i for i in range(min(50, len(val_df))) if i not in sample_indices]\n        sample_indices.extend(remaining[:6 - len(sample_indices)])\n\n    n_samples = len(sample_indices)\n    fig, axes = plt.subplots(n_samples, 4, figsize=(20, 4 * n_samples))\n    if n_samples == 1:\n        axes = axes.reshape(1, -1)\n\n    with torch.no_grad():\n        for row_idx, data_idx in enumerate(sample_indices):\n            row = val_df.iloc[data_idx]\n            image = np.array(Image.open(row['image_path']).convert('RGB'))\n            gt_mask = np.array(Image.open(row['mask_path']))\n            if gt_mask.ndim == 3:\n                gt_mask = gt_mask[:, :, 0]\n\n            # Run model\n            transformed = val_transform(image=image, mask=gt_mask)\n            img_tensor = transformed['image'].unsqueeze(0).to(DEVICE)\n            logits = model(img_tensor)\n            pred_mask = logits.argmax(dim=1).squeeze(0).cpu().numpy()\n\n            # Visualize\n            gt_rgb = mask_to_rgb(gt_mask, CLASS_COLORS)\n            pred_rgb = mask_to_rgb(pred_mask, CLASS_COLORS)\n            error_map = (gt_mask != pred_mask).astype(np.uint8) * 255\n\n            axes[row_idx, 0].imshow(image)\n            axes[row_idx, 0].set_title('Image', fontsize=10)\n            axes[row_idx, 1].imshow(gt_rgb)\n            axes[row_idx, 1].set_title('Ground Truth', fontsize=10)\n            axes[row_idx, 2].imshow(pred_rgb)\n            axes[row_idx, 2].set_title('Prediction', fontsize=10)\n            axes[row_idx, 3].imshow(error_map, cmap='gray')\n            axes[row_idx, 3].set_title('Error Map', fontsize=10)\n\n            for ax in axes[row_idx]:\n                ax.axis('off')\n\n    plt.suptitle('Validation Predictions (prioritizing images with weeds)',\n                 fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    # Legend\n    print('Mask colors:')\n    for cls_id, name in RICESEG_CLASSES.items():\n        r, g, b = CLASS_COLORS[cls_id]\n        print(f'  {cls_id}: {name} -- RGB({r}, {g}, {b})')\n\n    if USE_WANDB:\n        wandb.log({'predictions': wandb.Image(fig)})",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": "---\n## 8. Export Model\n\nSave the final model checkpoint for use in notebook 05 (inference pipeline)."
  },
  {
   "cell_type": "code",
   "id": "cell-42",
   "metadata": {},
   "source": "if best_model_path.exists():\n    model_size_mb = best_model_path.stat().st_size / (1024 * 1024)\n\n    print(f'=== Model Export Summary ===')\n    print(f'Model:          DeepLabV3+ (ResNet-50)')\n    print(f'Dataset:        RiceSEG ({NUM_CLASSES} classes)')\n    print(f'Best val mIoU:  {best_miou:.4f} ({best_miou*100:.1f}%)')\n    print(f'Checkpoint:     {best_model_path}')\n    print(f'Model size:     {model_size_mb:.1f} MB')\n    print(f'Input:          3xHxW RGB (any size, trained at {CROP_SIZE}x{CROP_SIZE})')\n    print(f'Output:         {NUM_CLASSES}xHxW logits (argmax -> class IDs)')\n    print(f'Normalization:  ImageNet (mean={IMAGENET_MEAN}, std={IMAGENET_STD})')\n\n    print(f'\\nCheckpoint contents:')\n    checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)\n    for key in checkpoint.keys():\n        print(f'  - {key}')\n\n    if USE_WANDB:\n        wandb.summary['best_val_miou'] = best_miou\n        wandb.summary['model_size_mb'] = model_size_mb\n        wandb.save(str(best_model_path))\n        wandb.finish()\n        print(f'\\nW&B run finished. Model artifact saved.')\n    elif 'wandb' in dir() and hasattr(wandb, 'run') and wandb.run is not None:\n        wandb.finish()\nelse:\n    print('No model checkpoint found.')\n    if USE_WANDB and hasattr(wandb, 'run') and wandb.run is not None:\n        wandb.finish()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": "---\n## 9. What's Next\n\n| Next Step | Notebook | What It Does |\n|-----------|----------|-------------|\n| **Inference pipeline** | `05-inference-pipeline.ipynb` | Run trained classification + segmentation models on new images |\n| **Improve weed IoU** | This notebook | Try: larger crop, more epochs, different backbone (ResNet-101), Dice+Focal combo loss |\n| **Classification** | `04-classification-baseline.ipynb` | Train EfficientNetV2-S on Crop & Weed or Bangladesh data |\n\n### Ideas to Improve Weed IoU\n\n1. **Dice + Focal combo loss:** Dice loss directly optimizes IoU. Combining it with focal loss often works better than either alone.\n2. **Larger backbone:** ResNet-101 or EfficientNet-B4 may capture finer weed textures.\n3. **Oversample weed images:** Create a weighted sampler that presents weed-containing images more often.\n4. **Philippines-only training:** Smaller dataset but most relevant to Indonesian conditions.\n5. **Test-time augmentation (TTA):** Average predictions from flipped/rotated versions of each image."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}