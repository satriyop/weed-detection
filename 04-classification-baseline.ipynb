{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 04 — Classification Baseline: EfficientNetV2-S — Rice Field Weed Detection\n\n**Purpose:** Train a classification model to identify weed species using contextually-relevant datasets.  \n**Runtime:** GPU required for training (Kaggle P100 recommended). Data prep runs on CPU.  \n**Platform:** Works on Kaggle, Colab, and local (with GPU).\n\n## What This Notebook Does\n\n1. **Setup** — Platform detection, dependency install, W&B logging init\n2. **Data pipeline** — Stratified train/val split, augmentation transforms, PyTorch DataLoaders\n3. **Model** — EfficientNetV2-S (pretrained ImageNet), dynamic classifier head\n4. **Training Phase 1** — Frozen backbone, train classifier head only (10 epochs)\n5. **Training Phase 2** — Unfreeze backbone, fine-tune end-to-end (10 epochs, lower LR)\n6. **Evaluation** — Confusion matrix, per-class F1, correct/misclassified examples\n7. **Export** — Save best model checkpoint\n\n### Dataset Options\n\n| Dataset | Classes | Expected Accuracy | Config Key |\n|---------|---------|-------------------|------------|\n| **Crop & Weed Detection** (default) | 2 (crop, weed) | 85-95% | `crop_weed_yolo` |\n| **Bangladesh Rice Field Weed** | 11 species | 65-80% (frozen) / 80-90% (fine-tuned) | `bangladesh_rice_weed` |\n\n**Default:** Crop & Weed Detection — available on Kaggle, no extra setup needed.  \n**Recommended upgrade:** Bangladesh Rice Field Weed — more classes, better species-level relevance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. Platform Detection & Setup\n\nSame pattern as notebook 01 — detect Kaggle vs Colab vs local, then set paths and install dependencies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\n# --- Platform Detection ---\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\ntry:\n    import google.colab\n    IS_COLAB = True\nexcept ImportError:\n    IS_COLAB = False\n\nIS_LOCAL = not IS_KAGGLE and not IS_COLAB\n\nPLATFORM = 'kaggle' if IS_KAGGLE else ('colab' if IS_COLAB else 'local')\nprint(f'Platform detected: {PLATFORM}')\nprint(f'Python version: {sys.version}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Install Training Dependencies\n\nThese are heavier than notebook 01 — we need PyTorch, `timm` (pretrained models), `albumentations` (augmentation), and `wandb` (experiment tracking).\n\n**On Kaggle/Colab:** PyTorch and torchvision are pre-installed. We only need `timm`, `albumentations`, and `wandb`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\n\npackages = ['timm', 'albumentations', 'wandb', 'scikit-learn']\n\nfor pkg in packages:\n    try:\n        __import__(pkg)\n    except ImportError:\n        print(f'Installing {pkg}...')\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n\nprint('All dependencies ready.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom collections import Counter\nfrom PIL import Image\nimport json\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\n\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport wandb\n\n# Consistent plot style\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['font.size'] = 11\n\n# Device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\nif DEVICE.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\nelse:\n    print('WARNING: No GPU detected. Training will be very slow.')\n\nprint('\\nImports ready.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### W&B Experiment Tracking (Optional)\n\nWeights & Biases logs training metrics, plots, and model artifacts to the cloud. This is optional but highly recommended for comparing runs.\n\n- **First time?** Get your API key at [wandb.ai/authorize](https://wandb.ai/authorize)\n- **Don't want W&B?** Set `USE_WANDB = False` below — metrics still print to stdout"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "USE_WANDB = True  # Set to False to skip W&B logging\n\nif USE_WANDB:\n    try:\n        wandb.login()\n        print('W&B login successful.')\n    except Exception as e:\n        print(f'W&B login failed: {e}')\n        print('Continuing without W&B. Set USE_WANDB = False to suppress this.')\n        USE_WANDB = False\nelse:\n    print('W&B disabled. Metrics will print to stdout only.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Load Dataset\n\nReuse the same dataset loading pattern from notebook 01. DeepWeeds provides a CSV mapping filenames to class labels (0-8)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATASET CONFIGURATION — Change ACTIVE_DATASET to switch\n# ============================================================\n\nDATASET_CONFIGS = {\n    'crop_weed_yolo': {\n        'name': 'Crop & Weed Detection',\n        'format': 'yolo',\n        'paths': {\n            'kaggle': '/kaggle/input/crop-and-weed-detection-data-with-bounding-boxes',\n            'colab': '/content/crop_weed_yolo',\n            'local': './data/crop_weed_yolo',\n        },\n    },\n    'bangladesh_rice_weed': {\n        'name': 'Bangladesh Rice Field Weed',\n        'format': 'folder',\n        'paths': {\n            'kaggle': '/kaggle/input/bangladesh-rice-field-weed',\n            'colab': '/content/bangladesh_rice_weed',\n            'local': './data/bangladesh_rice_weed',\n        },\n    },\n}\n\n# >>> CHANGE THIS to switch datasets <<<\nACTIVE_DATASET = 'crop_weed_yolo'\n\nconfig = DATASET_CONFIGS[ACTIVE_DATASET]\n\n# --- Dataset path ---\nif IS_KAGGLE:\n    DATA_ROOT = Path(config['paths']['kaggle'])\nelif IS_COLAB:\n    DATA_ROOT = Path(config['paths']['colab'])\nelse:\n    DATA_ROOT = Path(config['paths']['local'])\n\nprint(f'Active dataset: {config[\"name\"]}')\nprint(f'Format: {config[\"format\"]}')\nprint(f'Data root: {DATA_ROOT}')\nprint(f'Exists: {DATA_ROOT.exists()}')\n\nif not DATA_ROOT.exists():\n    print()\n    if ACTIVE_DATASET == 'crop_weed_yolo':\n        print('Add the dataset on Kaggle: search \"crop and weed detection\"')\n    elif ACTIVE_DATASET == 'bangladesh_rice_weed':\n        print('Upload from Mendeley Data: https://data.mendeley.com/datasets/mt72bmxz73/4')\n        print('Create a private Kaggle Dataset named \"bangladesh-rice-field-weed\"')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_image_path(filename):\n    \"\"\"Find the full path of an image file across different dataset structures.\"\"\"\n    p = Path(filename)\n    if p.is_absolute() and p.exists():\n        return str(p)\n    direct = DATA_ROOT / filename\n    if direct.exists():\n        return str(direct)\n    for subdir in ['images', 'train', 'data', 'img']:\n        candidate = DATA_ROOT / subdir / filename\n        if candidate.exists():\n            return str(candidate)\n    if DATA_ROOT.exists():\n        matches = list(DATA_ROOT.rglob(Path(filename).name))\n        if matches:\n            return str(matches[0])\n    return None\n\n\ndef load_classes_txt(data_root):\n    \"\"\"Load class names from classes.txt or similar files.\"\"\"\n    if not data_root.exists():\n        return None\n    for candidate in ['classes.txt', 'obj.names', 'data.names']:\n        for search_root in [data_root] + [d for d in data_root.iterdir() if d.is_dir()]:\n            path = search_root / candidate\n            if path.exists():\n                with open(path) as f:\n                    names = [line.strip() for line in f if line.strip()]\n                return {i: name for i, name in enumerate(names)}\n    return None\n\n\n# --- Load dataset based on format ---\ndf = pd.DataFrame()\nCLASS_NAMES = {}\nNUM_CLASSES = 0\nLABEL_COL = 'label'\n\nif not DATA_ROOT.exists():\n    print(f'Data directory not found: {DATA_ROOT}')\n    print('Remaining cells will show \"no data\" messages.')\nelif config['format'] == 'yolo':\n    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n    all_images = sorted(f for f in DATA_ROOT.rglob('*') if f.suffix.lower() in image_extensions)\n\n    CLASS_NAMES = load_classes_txt(DATA_ROOT)\n    if CLASS_NAMES is None:\n        CLASS_NAMES = {0: 'crop', 1: 'weed'}\n\n    rows = []\n    for img_path in all_images:\n        txt_path = img_path.with_suffix('.txt')\n        if not txt_path.exists():\n            rel = img_path.relative_to(DATA_ROOT)\n            parts = list(rel.parts)\n            for i, part in enumerate(parts):\n                if part.lower() in ('images', 'image', 'img'):\n                    parts[i] = 'labels'\n                    alt = DATA_ROOT / Path(*parts)\n                    alt = alt.with_suffix('.txt')\n                    if alt.exists():\n                        txt_path = alt\n                        break\n\n        if txt_path.exists():\n            with open(txt_path) as f:\n                class_ids = []\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) >= 5:\n                        class_ids.append(int(parts[0]))\n\n            if class_ids:\n                dominant_class = Counter(class_ids).most_common(1)[0][0]\n                rows.append({\n                    'image_path': str(img_path),\n                    ACTIVE_DATASET + '_label': dominant_class,\n                })\n\n    df = pd.DataFrame(rows)\n    LABEL_COL = ACTIVE_DATASET + '_label'\n\nelif config['format'] == 'folder':\n    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n    subdirs = sorted(d for d in DATA_ROOT.iterdir() if d.is_dir())\n\n    if len(subdirs) == 1:\n        deeper = subdirs[0]\n        deeper_subdirs = sorted(d for d in deeper.iterdir() if d.is_dir())\n        if len(deeper_subdirs) > 1:\n            subdirs = deeper_subdirs\n\n    CLASS_NAMES = {}\n    rows = []\n    for idx, subdir in enumerate(subdirs):\n        imgs = [f for f in subdir.iterdir() if f.suffix.lower() in image_extensions]\n        if imgs:\n            CLASS_NAMES[idx] = subdir.name\n            for img in imgs:\n                rows.append({'image_path': str(img), 'label': idx})\n\n    df = pd.DataFrame(rows)\n    LABEL_COL = 'label'\n\nNUM_CLASSES = len(CLASS_NAMES)\nCLASS_LIST = [CLASS_NAMES[i] for i in range(NUM_CLASSES)] if NUM_CLASSES > 0 else []\nHAS_DATA = len(df) > 0\n\nif HAS_DATA:\n    missing_mask = df['image_path'].apply(lambda p: not Path(p).exists())\n    if missing_mask.any():\n        print(f'WARNING: {missing_mask.sum()} images not found. Dropping them.')\n        df = df[~missing_mask].reset_index(drop=True)\n    print(f'Dataset: {config[\"name\"]}')\n    print(f'Images: {len(df)}')\n    print(f'Classes ({NUM_CLASSES}): {CLASS_NAMES}')\nelse:\n    print(f'No data loaded. Downstream cells will skip.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Class Distribution & Weight Calculation\n\nIf the class imbalance ratio is >3x, we use inverse-frequency weights in the loss function. This penalizes mistakes on rare classes more heavily, preventing the model from ignoring them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HAS_DATA:\n    class_weights = None\n    USE_WEIGHTED_LOSS = False\n    print('No data available for class distribution analysis.')\nelse:\n    class_counts = df[LABEL_COL].value_counts().sort_index()\n    imbalance_ratio = class_counts.max() / class_counts.min()\n\n    print('Class distribution:')\n    for idx, count in class_counts.items():\n        pct = count / len(df) * 100\n        name = CLASS_NAMES.get(idx, f'class_{idx}')\n        print(f'  {name:30s} {count:>5,} images ({pct:5.1f}%)')\n    print(f'\\nImbalance ratio: {imbalance_ratio:.1f}x')\n\n    USE_WEIGHTED_LOSS = imbalance_ratio > 3.0\n\n    if USE_WEIGHTED_LOSS:\n        total = len(df)\n        class_weights = torch.tensor(\n            [total / (NUM_CLASSES * class_counts.get(i, 1)) for i in range(NUM_CLASSES)],\n            dtype=torch.float32\n        )\n        class_weights = class_weights / class_weights.mean()\n        print(f'\\nUsing weighted loss. Weights:')\n        for i in range(NUM_CLASSES):\n            print(f'  {CLASS_NAMES.get(i, f\"class_{i}\"):30s} {class_weights[i]:.3f}')\n    else:\n        class_weights = None\n        print(f'\\nImbalance ratio <=3x. Using standard (unweighted) CrossEntropyLoss.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Train/Validation Split\n\n**Strategy:** 80% train / 20% validation, **stratified** by class label.\n\nStratified splitting ensures each split has roughly the same class proportions as the full dataset. Without this, a random split might put very few examples of a rare class into validation, making metrics unreliable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HAS_DATA:\n    train_df = pd.DataFrame()\n    val_df = pd.DataFrame()\n    print('No data available for splitting.')\nelse:\n    train_df, val_df = train_test_split(\n        df,\n        test_size=0.2,\n        stratify=df[LABEL_COL],\n        random_state=42\n    )\n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n\n    print(f'Train: {len(train_df):,} images')\n    print(f'Val:   {len(val_df):,} images')\n    print(f'\\nTrain class distribution:')\n    print(train_df[LABEL_COL].value_counts().sort_index())\n    print(f'\\nVal class distribution:')\n    print(val_df[LABEL_COL].value_counts().sort_index())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Data Pipeline\n\n### Augmentation Strategy\n\n**Training transforms** add variety to prevent overfitting:\n- **Geometric:** horizontal/vertical flip, rotation (30 deg), slight shift/scale\n- **Color:** brightness, contrast, hue, saturation jitter\n- **Normalize:** ImageNet statistics (required for pretrained backbone)\n\n**Validation transforms** only resize and normalize — no augmentation. We want a clean evaluation.\n\nWe use [Albumentations](https://albumentations.ai/) instead of torchvision transforms because it's faster and has more options."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "IMG_SIZE = 224  # EfficientNetV2-S default input size\n\n# ImageNet normalization (required for pretrained backbone)\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\ntrain_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),  # Resize first — images may vary in size (smartphone photos)\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.3),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),  # Resize — images may not be uniform size\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nprint(f'Image size: {IMG_SIZE}x{IMG_SIZE}')\nprint(f'Train augmentations: resize, flip, rotate, shift/scale, color jitter')\nprint(f'Val augmentations: resize + normalize only')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class WeedClassificationDataset(Dataset):\n    \"\"\"PyTorch dataset for weed classification.\"\"\"\n\n    def __init__(self, dataframe, label_col, transform=None):\n        self.df = dataframe.reset_index(drop=True)\n        self.label_col = label_col\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        img = Image.open(row['image_path']).convert('RGB')\n        img = np.array(img)\n\n        if self.transform:\n            img = self.transform(image=img)['image']\n\n        label = int(row[self.label_col])\n        return img, label\n\n\nif not HAS_DATA:\n    train_dataset = None\n    val_dataset = None\n    print('No data to create datasets.')\nelse:\n    train_dataset = WeedClassificationDataset(train_df, LABEL_COL, transform=train_transform)\n    val_dataset = WeedClassificationDataset(val_df, LABEL_COL, transform=val_transform)\n\n    print(f'Train dataset: {len(train_dataset)} samples')\n    print(f'Val dataset:   {len(val_dataset)} samples')\n\n    img, label = train_dataset[0]\n    print(f'\\nSample shape: {img.shape} (C, H, W)')\n    print(f'Sample label: {label} ({CLASS_NAMES.get(label, \"?\")})')\n    print(f'Pixel range: [{img.min():.2f}, {img.max():.2f}] (normalized)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "BATCH_SIZE = 32\nNUM_WORKERS = 2\n\nif not HAS_DATA:\n    train_loader = None\n    val_loader = None\n    print('No data for DataLoaders.')\nelse:\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        drop_last=True,\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    print(f'Batch size: {BATCH_SIZE}')\n    print(f'Train batches: {len(train_loader)}')\n    print(f'Val batches:   {len(val_loader)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Preview Augmented Samples\n\nLet's see what the training data looks like after augmentation. This is a sanity check — the images should look like plausible weed photos, not distorted beyond recognition."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HAS_DATA:\n    print('No data to preview augmented samples.')\nelse:\n    def denormalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n        \"\"\"Reverse ImageNet normalization for display.\"\"\"\n        mean = torch.tensor(mean).view(3, 1, 1)\n        std = torch.tensor(std).view(3, 1, 1)\n        return (tensor * std + mean).clamp(0, 1)\n\n    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n    for i, ax in enumerate(axes.flat):\n        img, label = train_dataset[i]\n        img_display = denormalize(img).permute(1, 2, 0).numpy()\n        ax.imshow(img_display)\n        ax.set_title(CLASS_NAMES[label], fontsize=9)\n        ax.axis('off')\n\n    plt.suptitle('Augmented Training Samples', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Model Setup\n\n### Why EfficientNetV2-S?\n\n| Property | Value |\n|----------|-------|\n| Parameters | ~21M |\n| ImageNet top-1 | ~84% |\n| Input size | 224x224 (default) |\n| Architecture | Fused-MBConv + MBConv blocks |\n| Training speed | 5-11x faster than EfficientNetV1 |\n\nEfficientNetV2-S is a strong modern baseline — accurate, fast, and small enough for a Kaggle P100. We load it with ImageNet-pretrained weights from `timm`.\n\n### Transfer Learning Strategy\n\n1. **Phase 1 (frozen):** Freeze backbone, only train the new classifier head. The backbone already knows generic image features (edges, textures, shapes) from ImageNet. We just need to teach the head which features correspond to which weed class.\n\n2. **Phase 2 (unfrozen):** Unfreeze the entire model, fine-tune with a lower learning rate. This lets the backbone adapt its features to weed-specific patterns (leaf shapes, stem textures, color profiles)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_model(num_classes, pretrained=True):\n    \"\"\"Create EfficientNetV2-S with a new classifier head.\"\"\"\n    model = timm.create_model(\n        'tf_efficientnetv2_s',\n        pretrained=pretrained,\n        num_classes=num_classes,\n    )\n    return model\n\n\ndef freeze_backbone(model):\n    \"\"\"Freeze all layers except the classifier head.\"\"\"\n    for param in model.parameters():\n        param.requires_grad = False\n    # Unfreeze the classifier head\n    for param in model.classifier.parameters():\n        param.requires_grad = True\n\n\ndef unfreeze_all(model):\n    \"\"\"Unfreeze all layers for end-to-end fine-tuning.\"\"\"\n    for param in model.parameters():\n        param.requires_grad = True\n\n\ndef count_parameters(model):\n    \"\"\"Count trainable and total parameters.\"\"\"\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    return trainable, total\n\n\n# Create model\nmodel = create_model(NUM_CLASSES, pretrained=True)\nmodel = model.to(DEVICE)\n\n# Start with frozen backbone\nfreeze_backbone(model)\ntrainable, total = count_parameters(model)\n\nprint(f'Model: EfficientNetV2-S')\nprint(f'Total parameters:     {total:>12,}')\nprint(f'Trainable parameters: {trainable:>12,} (classifier head only)')\nprint(f'Frozen parameters:    {total - trainable:>12,} (backbone)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Loss Function & Optimizer\n\n- **Loss:** CrossEntropyLoss (with class weights if imbalanced)\n- **Optimizer:** Adam with lr=1e-3 (Phase 1) or lr=1e-4 (Phase 2)\n- **Scheduler:** CosineAnnealingLR — smoothly decays learning rate to near-zero"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HAS_DATA:\n    criterion = nn.CrossEntropyLoss()\n    print('Loss: Standard CrossEntropyLoss (placeholder — no data)')\n    print('No data available. Skipping optimizer setup.')\n    LR_PHASE1 = 1e-3\n    EPOCHS_PHASE1 = 10\nelse:\n    if USE_WEIGHTED_LOSS and class_weights is not None:\n        criterion = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n        print('Loss: Weighted CrossEntropyLoss')\n    else:\n        criterion = nn.CrossEntropyLoss()\n        print('Loss: Standard CrossEntropyLoss')\n\n    LR_PHASE1 = 1e-3\n    EPOCHS_PHASE1 = 10\n\n    optimizer = optim.Adam(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=LR_PHASE1,\n    )\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE1)\n\n    print(f'\\nPhase 1 config:')\n    print(f'  Optimizer: Adam')\n    print(f'  Learning rate: {LR_PHASE1}')\n    print(f'  Epochs: {EPOCHS_PHASE1}')\n    print(f'  Scheduler: CosineAnnealingLR')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Training Loop\n\nThe training loop runs in two phases:\n\n| Phase | Backbone | LR | Epochs | Goal |\n|-------|----------|----|--------|---------|\n| 1 | Frozen | 1e-3 | 10 | Train classifier head on DeepWeeds features |\n| 2 | Unfrozen | 1e-4 | 10 | Fine-tune backbone for weed-specific features |\n\nPhase 2 only runs if Phase 1 accuracy is <60% OR you want to push accuracy higher."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_one_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch. Returns average loss and accuracy.\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (images, labels) in enumerate(loader):\n        images = images.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    avg_loss = running_loss / total\n    accuracy = correct / total\n    return avg_loss, accuracy\n\n\ndef validate(model, loader, criterion, device):\n    \"\"\"Validate the model. Returns average loss, accuracy, all predictions, and all labels.\"\"\"\n    model.train(False)  # set to evaluation mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = running_loss / total\n    accuracy = correct / total\n    return avg_loss, accuracy, np.array(all_preds), np.array(all_labels)\n\n\nprint('Training functions defined.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HAS_DATA:\n    USE_WANDB = False\n    print('No data. Skipping W&B init.')\nelif USE_WANDB:\n    wandb.init(\n        project='agri-weed-detection',\n        config={\n            'model': 'efficientnetv2_s',\n            'dataset': ACTIVE_DATASET,\n            'dataset_name': config['name'],\n            'img_size': IMG_SIZE,\n            'batch_size': BATCH_SIZE,\n            'lr_phase1': LR_PHASE1,\n            'epochs_phase1': EPOCHS_PHASE1,\n            'weighted_loss': USE_WEIGHTED_LOSS,\n            'num_classes': NUM_CLASSES,\n            'train_size': len(train_df),\n            'val_size': len(val_df),\n            'platform': PLATFORM,\n        },\n        tags=['baseline', 'classification', ACTIVE_DATASET],\n    )\n    print(f'W&B run: {wandb.run.name}')\n    print(f'W&B URL: {wandb.run.get_url()}')\nelse:\n    print('W&B disabled -- logging to stdout.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 1: Frozen Backbone\n\nOnly the classifier head trains. This is fast because we only backpropagate through the final layer. The backbone acts as a fixed feature extractor."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "SAVE_DIR = Path('/kaggle/working') if IS_KAGGLE else Path('./checkpoints')\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\nbest_val_acc = 0.0\nbest_model_path = SAVE_DIR / f'efficientnet_v2s_{ACTIVE_DATASET}_v1.pt'\n\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n\nif not HAS_DATA:\n    print('No training data. Skipping Phase 1.')\nelse:\n    print('=== Phase 1: Frozen Backbone ===')\n    print(f'Training classifier head only ({count_parameters(model)[0]:,} params)')\n    print(f'Checkpoint will save to: {best_model_path}\\n')\n\n    for epoch in range(EPOCHS_PHASE1):\n        t0 = time.time()\n\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n        val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n\n        scheduler.step()\n        current_lr = scheduler.get_last_lr()[0]\n        elapsed = time.time() - t0\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n        history['lr'].append(current_lr)\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'val_acc': val_acc,\n                'val_loss': val_loss,\n                'class_names': CLASS_NAMES,\n                'num_classes': NUM_CLASSES,\n                'img_size': IMG_SIZE,\n                'dataset': ACTIVE_DATASET,\n            }, best_model_path)\n            marker = ' * (best)'\n        else:\n            marker = ''\n\n        print(f'Epoch [{epoch+1:2d}/{EPOCHS_PHASE1}] '\n              f'train_loss={train_loss:.4f} train_acc={train_acc:.4f} | '\n              f'val_loss={val_loss:.4f} val_acc={val_acc:.4f} | '\n              f'lr={current_lr:.6f} | {elapsed:.1f}s{marker}')\n\n        if USE_WANDB:\n            wandb.log({\n                'epoch': epoch + 1,\n                'phase': 1,\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'train_acc': train_acc,\n                'val_acc': val_acc,\n                'lr': current_lr,\n            })\n\n    print(f'\\nPhase 1 complete. Best val accuracy: {best_val_acc:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 2: Unfreeze Backbone (Fine-Tuning)\n\nNow we unfreeze the entire model and train with a lower learning rate (1e-4). This lets the backbone adapt its features to weed-specific patterns.\n\n**Why lower LR?** The backbone weights are already well-trained on ImageNet. We want small, careful updates — large updates would destroy the learned features (\"catastrophic forgetting\").\n\n**When to skip Phase 2:** If Phase 1 accuracy is already >85%, Phase 2 may not add much. But running it is still recommended for best results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "LR_PHASE2 = 1e-4\nEPOCHS_PHASE2 = 10\n\nif not HAS_DATA:\n    print('No training data. Skipping Phase 2.')\nelse:\n    unfreeze_all(model)\n    trainable, total = count_parameters(model)\n\n    optimizer = optim.Adam(model.parameters(), lr=LR_PHASE2)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE2)\n\n    print('=== Phase 2: Full Fine-Tuning ===')\n    print(f'All parameters unfrozen ({trainable:,} trainable)\\n')\n\n    for epoch in range(EPOCHS_PHASE2):\n        t0 = time.time()\n        global_epoch = EPOCHS_PHASE1 + epoch\n\n        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n        val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n\n        scheduler.step()\n        current_lr = scheduler.get_last_lr()[0]\n        elapsed = time.time() - t0\n\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n        history['lr'].append(current_lr)\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': global_epoch,\n                'model_state_dict': model.state_dict(),\n                'val_acc': val_acc,\n                'val_loss': val_loss,\n                'class_names': CLASS_NAMES,\n                'num_classes': NUM_CLASSES,\n                'img_size': IMG_SIZE,\n            }, best_model_path)\n            marker = ' * (best)'\n        else:\n            marker = ''\n\n        print(f'Epoch [{global_epoch+1:2d}/{EPOCHS_PHASE1 + EPOCHS_PHASE2}] '\n              f'train_loss={train_loss:.4f} train_acc={train_acc:.4f} | '\n              f'val_loss={val_loss:.4f} val_acc={val_acc:.4f} | '\n              f'lr={current_lr:.6f} | {elapsed:.1f}s{marker}')\n\n        if USE_WANDB:\n            wandb.log({\n                'epoch': global_epoch + 1,\n                'phase': 2,\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'train_acc': train_acc,\n                'val_acc': val_acc,\n                'lr': current_lr,\n            })\n\n    print(f'\\nPhase 2 complete. Best val accuracy: {best_val_acc:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Training History Plot\n\nVisualize how loss and accuracy evolved across both phases."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not history['train_loss']:\n    print('No training history to plot.')\nelse:\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    epochs_range = range(1, len(history['train_loss']) + 1)\n    phase_boundary = EPOCHS_PHASE1\n\n    axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train')\n    axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val')\n    axes[0].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Loss')\n    axes[0].legend()\n\n    axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train')\n    axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val')\n    axes[1].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].set_title('Accuracy')\n    axes[1].legend()\n\n    axes[2].plot(epochs_range, history['lr'], 'g-')\n    axes[2].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\n    axes[2].set_xlabel('Epoch')\n    axes[2].set_ylabel('Learning Rate')\n    axes[2].set_title('Learning Rate Schedule')\n    axes[2].legend()\n\n    plt.suptitle('Training History (Phase 1: frozen -> Phase 2: unfrozen)', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Evaluation\n\nLoad the best checkpoint and run a thorough evaluation:\n- Confusion matrix\n- Per-class precision, recall, F1\n- Correctly classified and misclassified examples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HAS_DATA or not best_model_path.exists():\n    print('No model checkpoint found. Skipping evaluation.')\n    val_preds = np.array([])\n    val_labels = np.array([])\nelse:\n    checkpoint = torch.load(best_model_path, map_location=DEVICE, weights_only=False)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f'Loaded best model from epoch {checkpoint[\"epoch\"] + 1}')\n    print(f'Best val accuracy: {checkpoint[\"val_acc\"]:.4f}')\n    print(f'Best val loss: {checkpoint[\"val_loss\"]:.4f}')\n\n    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n    print(f'\\nFinal validation: loss={val_loss:.4f}, accuracy={val_acc:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Confusion Matrix\n\nThe confusion matrix shows which classes the model confuses. Diagonal entries = correct predictions. Off-diagonal = mistakes.\n\n**What to look for:**\n- Large off-diagonal values = classes the model confuses (visually similar?)\n- A row with low diagonal value = a class the model struggles to recognize\n- A column with many values = a class the model over-predicts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(val_preds) == 0:\n    print('No predictions available. Skipping confusion matrix.')\nelse:\n    cm = confusion_matrix(val_labels, val_preds)\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n    im0 = axes[0].imshow(cm, interpolation='nearest', cmap='Blues')\n    axes[0].set_title('Confusion Matrix (Counts)')\n    for i in range(NUM_CLASSES):\n        for j in range(NUM_CLASSES):\n            axes[0].text(j, i, str(cm[i, j]), ha='center', va='center',\n                         color='white' if cm[i, j] > cm.max() / 2 else 'black', fontsize=8)\n\n    im1 = axes[1].imshow(cm_normalized, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\n    axes[1].set_title('Confusion Matrix (Normalized by Row = Recall)')\n    for i in range(NUM_CLASSES):\n        for j in range(NUM_CLASSES):\n            axes[1].text(j, i, f'{cm_normalized[i, j]:.2f}', ha='center', va='center',\n                         color='white' if cm_normalized[i, j] > 0.5 else 'black', fontsize=8)\n\n    for ax in axes:\n        ax.set_xticks(range(NUM_CLASSES))\n        ax.set_yticks(range(NUM_CLASSES))\n        ax.set_xticklabels(CLASS_LIST, rotation=45, ha='right', fontsize=9)\n        ax.set_yticklabels(CLASS_LIST, fontsize=9)\n        ax.set_xlabel('Predicted')\n        ax.set_ylabel('Actual')\n\n    fig.colorbar(im0, ax=axes[0], fraction=0.046)\n    fig.colorbar(im1, ax=axes[1], fraction=0.046)\n    plt.suptitle('Classification Results', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n    if USE_WANDB:\n        wandb.log({'confusion_matrix': wandb.Image(fig)})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(val_preds) == 0:\n    print('No predictions available. Skipping classification report.')\nelse:\n    report = classification_report(\n        val_labels, val_preds,\n        target_names=CLASS_LIST,\n        digits=3,\n        output_dict=True\n    )\n\n    print('Per-Class Classification Report:')\n    print('=' * 70)\n    print(classification_report(val_labels, val_preds, target_names=CLASS_LIST, digits=3))\n\n    f1_scores = [report[name]['f1-score'] for name in CLASS_LIST]\n    colors = ['green' if f1 > 0.8 else 'orange' if f1 > 0.6 else 'red' for f1 in f1_scores]\n\n    fig, ax = plt.subplots(figsize=(12, 5))\n    bars = ax.barh(CLASS_LIST, f1_scores, color=colors)\n    ax.set_xlabel('F1 Score')\n    ax.set_title('Per-Class F1 Score')\n    ax.set_xlim(0, 1)\n    ax.axvline(x=0.8, color='gray', linestyle='--', alpha=0.5, label='Good threshold')\n\n    for bar, f1 in zip(bars, f1_scores):\n        ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n                f'{f1:.3f}', va='center', fontsize=10)\n\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n    if USE_WANDB:\n        wandb.log({'f1_per_class': wandb.Image(fig)})\n        for name in CLASS_LIST:\n            wandb.log({f'f1_{name}': report[name]['f1-score']})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Correct vs Misclassified Examples\n\nLooking at what the model gets right and wrong is more informative than any metric. Pay attention to:\n- **Misclassified images:** Are they genuinely ambiguous? Mislabeled? Or a clear model failure?\n- **Which classes get confused?** Cross-reference with the confusion matrix."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if len(val_preds) == 0:\n    print('No predictions available. Skipping examples display.')\nelse:\n    correct_indices = np.where(val_preds == val_labels)[0]\n    incorrect_indices = np.where(val_preds != val_labels)[0]\n\n    print(f'Correct:   {len(correct_indices):,} ({len(correct_indices)/len(val_labels)*100:.1f}%)')\n    print(f'Incorrect: {len(incorrect_indices):,} ({len(incorrect_indices)/len(val_labels)*100:.1f}%)')\n\n\n    def show_predictions(indices, title, n=5):\n        \"\"\"Show predicted vs actual labels for a set of image indices.\"\"\"\n        if len(indices) == 0:\n            print(f'{title}: no examples to show.')\n            return\n        np.random.seed(42)\n        selected = np.random.choice(indices, size=min(n, len(indices)), replace=False)\n\n        fig, axes = plt.subplots(1, n, figsize=(3 * n, 4))\n        if n == 1:\n            axes = [axes]\n\n        for ax, idx in zip(axes, selected):\n            img_path = val_df.iloc[idx]['image_path']\n            img = Image.open(img_path).convert('RGB')\n            ax.imshow(img)\n\n            pred_name = CLASS_NAMES[val_preds[idx]]\n            true_name = CLASS_NAMES[val_labels[idx]]\n\n            if val_preds[idx] == val_labels[idx]:\n                ax.set_title(f'Pred: {pred_name}\\nTrue: {true_name}', fontsize=9, color='green')\n            else:\n                ax.set_title(f'Pred: {pred_name}\\nTrue: {true_name}', fontsize=9, color='red')\n            ax.axis('off')\n\n        plt.suptitle(title, fontsize=13, fontweight='bold')\n        plt.tight_layout()\n        plt.show()\n\n\n    show_predictions(correct_indices, 'Correctly Classified Examples', n=5)\n    if len(incorrect_indices) > 0:\n        show_predictions(incorrect_indices, 'Misclassified Examples', n=5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. Export Model\n\nSave the final model checkpoint for use in notebook 05 (inference pipeline)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not HAS_DATA or not best_model_path.exists():\n    print('No model checkpoint found.')\n    print('Train with data on Kaggle/Colab to generate a checkpoint.')\nelse:\n    model_size_mb = best_model_path.stat().st_size / (1024 * 1024)\n\n    print(f'=== Model Export Summary ===')\n    print(f'Model:          EfficientNetV2-S')\n    print(f'Dataset:        {config[\"name\"]} ({NUM_CLASSES} classes)')\n    print(f'Best val acc:   {best_val_acc:.4f} ({best_val_acc*100:.1f}%)')\n    print(f'Checkpoint:     {best_model_path}')\n    print(f'Model size:     {model_size_mb:.1f} MB')\n    print(f'Input size:     {IMG_SIZE}x{IMG_SIZE} RGB')\n    print(f'Normalization:  ImageNet (mean={IMAGENET_MEAN}, std={IMAGENET_STD})')\n    print(f'\\nCheckpoint contents:')\n    for key in checkpoint.keys():\n        print(f'  - {key}')\n\n    if USE_WANDB:\n        wandb.summary['best_val_acc'] = best_val_acc\n        wandb.summary['model_size_mb'] = model_size_mb\n        wandb.summary['dataset'] = ACTIVE_DATASET\n        wandb.save(str(best_model_path))\n        wandb.finish()\n        print(f'\\nW&B run finished. Model artifact saved.')\n\n    print(f'\\n=== Next Steps ===')\n    print(f'1. Download {best_model_path.name} from Kaggle output tab (if needed locally)')\n    print(f'2. Notebook 05: Build inference pipeline using this checkpoint')\n    print(f'3. Notebook 02: Explore RiceSEG for segmentation training')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}