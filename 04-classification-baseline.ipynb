{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 04 \u2014 Classification Baseline: EfficientNetV2-S \u2014 Rice Field Weed Detection\n\n**Purpose:** Train a classification model to identify weed species using contextually-relevant datasets.  \n**Runtime:** GPU required for training (Kaggle P100 recommended). Data prep runs on CPU.  \n**Platform:** Works on Kaggle, Colab, and local (with GPU).\n\n## What This Notebook Does\n\n1. **Setup** \u2014 Platform detection, dependency install, W&B logging init\n2. **Data pipeline** \u2014 Stratified train/val split, augmentation transforms, PyTorch DataLoaders\n3. **Model** \u2014 EfficientNetV2-S (pretrained ImageNet), dynamic classifier head\n4. **Training Phase 1** \u2014 Frozen backbone, train classifier head only (10 epochs)\n5. **Training Phase 2** \u2014 Unfreeze backbone, fine-tune end-to-end (10 epochs, lower LR)\n6. **Evaluation** \u2014 Confusion matrix, per-class F1, correct/misclassified examples\n7. **Export** \u2014 Save best model checkpoint\n\n### Dataset Options\n\n| Dataset | Classes | Expected Accuracy | Config Key |\n|---------|---------|-------------------|------------|\n| **Crop & Weed Detection** (default) | 2 (crop, weed) | 85-95% | `crop_weed_yolo` |\n| **Bangladesh Rice Field Weed** | 11 species | 65-80% (frozen) / 80-90% (fine-tuned) | `bangladesh_rice_weed` |\n\n**Default:** Crop & Weed Detection \u2014 available on Kaggle, no extra setup needed.  \n**Recommended upgrade:** Bangladesh Rice Field Weed \u2014 more classes, better species-level relevance."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. Platform Detection & Setup\n\nSame pattern as notebook 01 \u2014 detect Kaggle vs Colab vs local, then set paths and install dependencies."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\n\n# --- Platform Detection ---\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\ntry:\n    import google.colab\n    IS_COLAB = True\nexcept ImportError:\n    IS_COLAB = False\n\nIS_LOCAL = not IS_KAGGLE and not IS_COLAB\n\nPLATFORM = 'kaggle' if IS_KAGGLE else ('colab' if IS_COLAB else 'local')\nprint(f'Platform detected: {PLATFORM}')\nprint(f'Python version: {sys.version}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Install Training Dependencies\n\nThese are heavier than notebook 01 \u2014 we need PyTorch, `timm` (pretrained models), `albumentations` (augmentation), and `wandb` (experiment tracking).\n\n**On Kaggle/Colab:** PyTorch and torchvision are pre-installed. We only need `timm`, `albumentations`, and `wandb`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\n\npackages = ['timm', 'albumentations', 'wandb', 'scikit-learn']\n\nfor pkg in packages:\n    try:\n        __import__(pkg)\n    except ImportError:\n        print(f'Installing {pkg}...')\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n\nprint('All dependencies ready.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom collections import Counter\nfrom PIL import Image\nimport json\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\n\nimport timm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport wandb\n\n# Consistent plot style\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['font.size'] = 11\n\n# Device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\nif DEVICE.type == 'cuda':\n    print(f'GPU: {torch.cuda.get_device_name(0)}')\n    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB')\nelse:\n    print('WARNING: No GPU detected. Training will be very slow.')\n\nprint('\\nImports ready.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### W&B Experiment Tracking (Optional)\n\nWeights & Biases logs training metrics, plots, and model artifacts to the cloud. This is optional but highly recommended for comparing runs.\n\n- **First time?** Get your API key at [wandb.ai/authorize](https://wandb.ai/authorize)\n- **Don't want W&B?** Set `USE_WANDB = False` below \u2014 metrics still print to stdout"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "USE_WANDB = True  # Set to False to skip W&B logging\n\nif USE_WANDB:\n    try:\n        wandb.login()\n        print('W&B login successful.')\n    except Exception as e:\n        print(f'W&B login failed: {e}')\n        print('Continuing without W&B. Set USE_WANDB = False to suppress this.')\n        USE_WANDB = False\nelse:\n    print('W&B disabled. Metrics will print to stdout only.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Load Dataset\n\nReuse the same dataset loading pattern from notebook 01. DeepWeeds provides a CSV mapping filenames to class labels (0-8)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# DATASET CONFIGURATION \u2014 Change ACTIVE_DATASET to switch\n# ============================================================\n\nDATASET_CONFIGS = {\n    'crop_weed_yolo': {\n        'name': 'Crop & Weed Detection',\n        'format': 'yolo',\n        'paths': {\n            'kaggle': '/kaggle/input/crop-and-weed-detection-data-with-bounding-boxes',\n            'colab': '/content/crop_weed_yolo',\n            'local': './data/crop_weed_yolo',\n        },\n    },\n    'bangladesh_rice_weed': {\n        'name': 'Bangladesh Rice Field Weed',\n        'format': 'folder',\n        'paths': {\n            'kaggle': '/kaggle/input/bangladesh-rice-field-weed',\n            'colab': '/content/bangladesh_rice_weed',\n            'local': './data/bangladesh_rice_weed',\n        },\n    },\n}\n\n# >>> CHANGE THIS to switch datasets <<<\nACTIVE_DATASET = 'crop_weed_yolo'\n\nconfig = DATASET_CONFIGS[ACTIVE_DATASET]\n\n# --- Dataset path ---\nif IS_KAGGLE:\n    DATA_ROOT = Path(config['paths']['kaggle'])\nelif IS_COLAB:\n    DATA_ROOT = Path(config['paths']['colab'])\nelse:\n    DATA_ROOT = Path(config['paths']['local'])\n\nprint(f'Active dataset: {config[\"name\"]}')\nprint(f'Format: {config[\"format\"]}')\nprint(f'Data root: {DATA_ROOT}')\nprint(f'Exists: {DATA_ROOT.exists()}')\n\nif not DATA_ROOT.exists():\n    print()\n    if ACTIVE_DATASET == 'crop_weed_yolo':\n        print('Add the dataset on Kaggle: search \"crop and weed detection\"')\n    elif ACTIVE_DATASET == 'bangladesh_rice_weed':\n        print('Upload from Mendeley Data: https://data.mendeley.com/datasets/mt72bmxz73/4')\n        print('Create a private Kaggle Dataset named \"bangladesh-rice-field-weed\"')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_image_path(filename):\n    \"\"\"Find the full path of an image file across different dataset structures.\"\"\"\n    p = Path(filename)\n    if p.is_absolute() and p.exists():\n        return str(p)\n    direct = DATA_ROOT / filename\n    if direct.exists():\n        return str(direct)\n    for subdir in ['images', 'train', 'data', 'img']:\n        candidate = DATA_ROOT / subdir / filename\n        if candidate.exists():\n            return str(candidate)\n    matches = list(DATA_ROOT.rglob(Path(filename).name))\n    if matches:\n        return str(matches[0])\n    return None\n\n\ndef load_classes_txt(data_root):\n    \"\"\"Load class names from classes.txt or similar files.\"\"\"\n    if not data_root.exists():\n        return None\n    for candidate in ['classes.txt', 'obj.names', 'data.names']:\n        for search_root in [data_root] + [d for d in data_root.iterdir() if d.is_dir()]:\n            path = search_root / candidate if not isinstance(search_root, Path) else search_root / candidate\n            if path.exists():\n                with open(path) as f:\n                    names = [line.strip() for line in f if line.strip()]\n                return {i: name for i, name in enumerate(names)}\n    return None\n\n\n# --- Load dataset based on format ---\nif config['format'] == 'yolo':\n    # For classification from YOLO: each image gets its dominant class\n    # Parse annotations to determine per-image class\n    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n    all_images = sorted(f for f in DATA_ROOT.rglob('*') if f.suffix.lower() in image_extensions)\n    \n    CLASS_NAMES = load_classes_txt(DATA_ROOT)\n    if CLASS_NAMES is None:\n        CLASS_NAMES = {0: 'crop', 1: 'weed'}\n    \n    rows = []\n    for img_path in all_images:\n        txt_path = img_path.with_suffix('.txt')\n        if not txt_path.exists():\n            # Try labels/ directory parallel to images/\n            rel = img_path.relative_to(DATA_ROOT)\n            parts = list(rel.parts)\n            for i, part in enumerate(parts):\n                if part.lower() in ('images', 'image', 'img'):\n                    parts[i] = 'labels'\n                    alt = DATA_ROOT / Path(*parts)\n                    alt = alt.with_suffix('.txt')\n                    if alt.exists():\n                        txt_path = alt\n                        break\n        \n        if txt_path.exists():\n            with open(txt_path) as f:\n                class_ids = []\n                for line in f:\n                    parts = line.strip().split()\n                    if len(parts) >= 5:\n                        class_ids.append(int(parts[0]))\n            \n            if class_ids:\n                # Use most frequent class in the image as the label\n                dominant_class = Counter(class_ids).most_common(1)[0][0]\n                rows.append({\n                    'image_path': str(img_path),\n                    ACTIVE_DATASET + '_label': dominant_class,\n                })\n    \n    df = pd.DataFrame(rows)\n    LABEL_COL = ACTIVE_DATASET + '_label'\n\nelif config['format'] == 'folder':\n    # Walk subdirectories\n    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n    subdirs = sorted(d for d in DATA_ROOT.iterdir() if d.is_dir())\n    \n    # Check one level deeper\n    if len(subdirs) == 1:\n        deeper = subdirs[0]\n        deeper_subdirs = sorted(d for d in deeper.iterdir() if d.is_dir())\n        if len(deeper_subdirs) > 1:\n            subdirs = deeper_subdirs\n    \n    CLASS_NAMES = {}\n    rows = []\n    for idx, subdir in enumerate(subdirs):\n        imgs = [f for f in subdir.iterdir() if f.suffix.lower() in image_extensions]\n        if imgs:\n            CLASS_NAMES[idx] = subdir.name\n            for img in imgs:\n                rows.append({'image_path': str(img), 'label': idx})\n    \n    df = pd.DataFrame(rows)\n    LABEL_COL = 'label'\n\nNUM_CLASSES = len(CLASS_NAMES)\nCLASS_LIST = [CLASS_NAMES[i] for i in range(NUM_CLASSES)]\n\n# Drop rows where images weren't found\nif 'image_path' in df.columns:\n    missing_mask = df['image_path'].apply(lambda p: not Path(p).exists())\n    if missing_mask.any():\n        print(f'WARNING: {missing_mask.sum()} images not found. Dropping them.')\n        df = df[~missing_mask].reset_index(drop=True)\n\nprint(f'Dataset: {config[\"name\"]}')\nprint(f'Images: {len(df)}')\nprint(f'Classes ({NUM_CLASSES}): {CLASS_NAMES}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Class Distribution & Weight Calculation\n\nIf the class imbalance ratio is >3x, we use inverse-frequency weights in the loss function. This penalizes mistakes on rare classes more heavily, preventing the model from ignoring them."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class_counts = df[LABEL_COL].value_counts().sort_index()\nimbalance_ratio = class_counts.max() / class_counts.min()\n\nprint('Class distribution:')\nfor idx, count in class_counts.items():\n    pct = count / len(df) * 100\n    name = CLASS_NAMES.get(idx, f'class_{idx}')\n    print(f'  {name:30s} {count:>5,} images ({pct:5.1f}%)')\nprint(f'\\nImbalance ratio: {imbalance_ratio:.1f}x')\n\n# Compute class weights (inverse frequency)\nUSE_WEIGHTED_LOSS = imbalance_ratio > 3.0\n\nif USE_WEIGHTED_LOSS:\n    total = len(df)\n    class_weights = torch.tensor(\n        [total / (NUM_CLASSES * class_counts.get(i, 1)) for i in range(NUM_CLASSES)],\n        dtype=torch.float32\n    )\n    class_weights = class_weights / class_weights.mean()\n    print(f'\\nUsing weighted loss. Weights:')\n    for i in range(NUM_CLASSES):\n        print(f'  {CLASS_NAMES.get(i, f\"class_{i}\"):30s} {class_weights[i]:.3f}')\nelse:\n    class_weights = None\n    print(f'\\nImbalance ratio <=3x. Using standard (unweighted) CrossEntropyLoss.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Train/Validation Split\n\n**Strategy:** 80% train / 20% validation, **stratified** by class label.\n\nStratified splitting ensures each split has roughly the same class proportions as the full dataset. Without this, a random split might put very few examples of a rare class into validation, making metrics unreliable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train_df, val_df = train_test_split(\n    df, \n    test_size=0.2, \n    stratify=df[LABEL_COL], \n    random_state=42\n)\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\n\nprint(f'Train: {len(train_df):,} images')\nprint(f'Val:   {len(val_df):,} images')\nprint(f'\\nTrain class distribution:')\nprint(train_df[LABEL_COL].value_counts().sort_index())\nprint(f'\\nVal class distribution:')\nprint(val_df[LABEL_COL].value_counts().sort_index())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Data Pipeline\n\n### Augmentation Strategy\n\n**Training transforms** add variety to prevent overfitting:\n- **Geometric:** horizontal/vertical flip, rotation (30 deg), slight shift/scale\n- **Color:** brightness, contrast, hue, saturation jitter\n- **Normalize:** ImageNet statistics (required for pretrained backbone)\n\n**Validation transforms** only resize and normalize \u2014 no augmentation. We want a clean evaluation.\n\nWe use [Albumentations](https://albumentations.ai/) instead of torchvision transforms because it's faster and has more options."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "IMG_SIZE = 224  # EfficientNetV2-S default input size\n\n# ImageNet normalization (required for pretrained backbone)\nIMAGENET_MEAN = [0.485, 0.456, 0.406]\nIMAGENET_STD = [0.229, 0.224, 0.225]\n\ntrain_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),  # Resize first \u2014 images may vary in size (smartphone photos)\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.Rotate(limit=30, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.3),\n    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Resize(IMG_SIZE, IMG_SIZE),  # Resize \u2014 images may not be uniform size\n    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n    ToTensorV2(),\n])\n\nprint(f'Image size: {IMG_SIZE}x{IMG_SIZE}')\nprint(f'Train augmentations: resize, flip, rotate, shift/scale, color jitter')\nprint(f'Val augmentations: resize + normalize only')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class WeedClassificationDataset(Dataset):\n    \"\"\"PyTorch dataset for weed classification.\"\"\"\n    \n    def __init__(self, dataframe, label_col, transform=None):\n        self.df = dataframe.reset_index(drop=True)\n        self.label_col = label_col\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        # Load image as RGB numpy array\n        img = Image.open(row['image_path']).convert('RGB')\n        img = np.array(img)\n        \n        # Apply augmentation\n        if self.transform:\n            img = self.transform(image=img)['image']\n        \n        label = int(row[self.label_col])\n        return img, label\n\n\n# Create datasets\ntrain_dataset = WeedClassificationDataset(train_df, LABEL_COL, transform=train_transform)\nval_dataset = WeedClassificationDataset(val_df, LABEL_COL, transform=val_transform)\n\nprint(f'Train dataset: {len(train_dataset)} samples')\nprint(f'Val dataset:   {len(val_dataset)} samples')\n\n# Sanity check: load one sample\nimg, label = train_dataset[0]\nprint(f'\\nSample shape: {img.shape} (C, H, W)')\nprint(f'Sample label: {label} ({CLASS_NAMES.get(label, \"?\")})')\nprint(f'Pixel range: [{img.min():.2f}, {img.max():.2f}] (normalized)')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# DataLoaders\nBATCH_SIZE = 32\nNUM_WORKERS = 2  # Kaggle has 2 CPU cores\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n    drop_last=True,  # Drop incomplete last batch for stable batch norm\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True,\n)\n\nprint(f'Batch size: {BATCH_SIZE}')\nprint(f'Train batches: {len(train_loader)}')\nprint(f'Val batches:   {len(val_loader)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Preview Augmented Samples\n\nLet's see what the training data looks like after augmentation. This is a sanity check \u2014 the images should look like plausible weed photos, not distorted beyond recognition."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def denormalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):\n    \"\"\"Reverse ImageNet normalization for display.\"\"\"\n    mean = torch.tensor(mean).view(3, 1, 1)\n    std = torch.tensor(std).view(3, 1, 1)\n    return (tensor * std + mean).clamp(0, 1)\n\n\nfig, axes = plt.subplots(2, 6, figsize=(18, 6))\nfor i, ax in enumerate(axes.flat):\n    img, label = train_dataset[i]\n    img_display = denormalize(img).permute(1, 2, 0).numpy()\n    ax.imshow(img_display)\n    ax.set_title(CLASS_NAMES[label], fontsize=9)\n    ax.axis('off')\n\nplt.suptitle('Augmented Training Samples', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Model Setup\n\n### Why EfficientNetV2-S?\n\n| Property | Value |\n|----------|-------|\n| Parameters | ~21M |\n| ImageNet top-1 | ~84% |\n| Input size | 224x224 (default) |\n| Architecture | Fused-MBConv + MBConv blocks |\n| Training speed | 5-11x faster than EfficientNetV1 |\n\nEfficientNetV2-S is a strong modern baseline \u2014 accurate, fast, and small enough for a Kaggle P100. We load it with ImageNet-pretrained weights from `timm`.\n\n### Transfer Learning Strategy\n\n1. **Phase 1 (frozen):** Freeze backbone, only train the new classifier head. The backbone already knows generic image features (edges, textures, shapes) from ImageNet. We just need to teach the head which features correspond to which weed class.\n\n2. **Phase 2 (unfrozen):** Unfreeze the entire model, fine-tune with a lower learning rate. This lets the backbone adapt its features to weed-specific patterns (leaf shapes, stem textures, color profiles)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_model(num_classes, pretrained=True):\n    \"\"\"Create EfficientNetV2-S with a new classifier head.\"\"\"\n    model = timm.create_model(\n        'tf_efficientnetv2_s',\n        pretrained=pretrained,\n        num_classes=num_classes,\n    )\n    return model\n\n\ndef freeze_backbone(model):\n    \"\"\"Freeze all layers except the classifier head.\"\"\"\n    for param in model.parameters():\n        param.requires_grad = False\n    # Unfreeze the classifier head\n    for param in model.classifier.parameters():\n        param.requires_grad = True\n\n\ndef unfreeze_all(model):\n    \"\"\"Unfreeze all layers for end-to-end fine-tuning.\"\"\"\n    for param in model.parameters():\n        param.requires_grad = True\n\n\ndef count_parameters(model):\n    \"\"\"Count trainable and total parameters.\"\"\"\n    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total = sum(p.numel() for p in model.parameters())\n    return trainable, total\n\n\n# Create model\nmodel = create_model(NUM_CLASSES, pretrained=True)\nmodel = model.to(DEVICE)\n\n# Start with frozen backbone\nfreeze_backbone(model)\ntrainable, total = count_parameters(model)\n\nprint(f'Model: EfficientNetV2-S')\nprint(f'Total parameters:     {total:>12,}')\nprint(f'Trainable parameters: {trainable:>12,} (classifier head only)')\nprint(f'Frozen parameters:    {total - trainable:>12,} (backbone)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Loss Function & Optimizer\n\n- **Loss:** CrossEntropyLoss (with class weights if imbalanced)\n- **Optimizer:** Adam with lr=1e-3 (Phase 1) or lr=1e-4 (Phase 2)\n- **Scheduler:** CosineAnnealingLR \u2014 smoothly decays learning rate to near-zero"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Loss function ---\nif USE_WEIGHTED_LOSS and class_weights is not None:\n    criterion = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n    print('Loss: Weighted CrossEntropyLoss')\nelse:\n    criterion = nn.CrossEntropyLoss()\n    print('Loss: Standard CrossEntropyLoss')\n\n# --- Phase 1 optimizer (frozen backbone) ---\nLR_PHASE1 = 1e-3\nEPOCHS_PHASE1 = 10\n\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=LR_PHASE1,\n)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE1)\n\nprint(f'\\nPhase 1 config:')\nprint(f'  Optimizer: Adam')\nprint(f'  Learning rate: {LR_PHASE1}')\nprint(f'  Epochs: {EPOCHS_PHASE1}')\nprint(f'  Scheduler: CosineAnnealingLR')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Training Loop\n\nThe training loop runs in two phases:\n\n| Phase | Backbone | LR | Epochs | Goal |\n|-------|----------|----|--------|---------|\n| 1 | Frozen | 1e-3 | 10 | Train classifier head on DeepWeeds features |\n| 2 | Unfrozen | 1e-4 | 10 | Fine-tune backbone for weed-specific features |\n\nPhase 2 only runs if Phase 1 accuracy is <60% OR you want to push accuracy higher."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_one_epoch(model, loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch. Returns average loss and accuracy.\"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (images, labels) in enumerate(loader):\n        images = images.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * images.size(0)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n    \n    avg_loss = running_loss / total\n    accuracy = correct / total\n    return avg_loss, accuracy\n\n\ndef validate(model, loader, criterion, device):\n    \"\"\"Validate the model. Returns average loss, accuracy, all predictions, and all labels.\"\"\"\n    model.train(False)  # set to evaluation mode\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item() * images.size(0)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = running_loss / total\n    accuracy = correct / total\n    return avg_loss, accuracy, np.array(all_preds), np.array(all_labels)\n\n\nprint('Training functions defined.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- W&B run init ---\nif USE_WANDB:\n    wandb.init(\n        project='agri-weed-detection',\n        config={\n            'model': 'efficientnetv2_s',\n            'dataset': ACTIVE_DATASET,\n            'dataset_name': config['name'],\n            'img_size': IMG_SIZE,\n            'batch_size': BATCH_SIZE,\n            'lr_phase1': LR_PHASE1,\n            'epochs_phase1': EPOCHS_PHASE1,\n            'weighted_loss': USE_WEIGHTED_LOSS,\n            'num_classes': NUM_CLASSES,\n            'train_size': len(train_df),\n            'val_size': len(val_df),\n            'platform': PLATFORM,\n        },\n        tags=['baseline', 'classification', ACTIVE_DATASET],\n    )\n    print(f'W&B run: {wandb.run.name}')\n    print(f'W&B URL: {wandb.run.get_url()}')\nelse:\n    print('W&B disabled -- logging to stdout.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 1: Frozen Backbone\n\nOnly the classifier head trains. This is fast because we only backpropagate through the final layer. The backbone acts as a fixed feature extractor."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Checkpoint tracking ---\nSAVE_DIR = Path('/kaggle/working') if IS_KAGGLE else Path('./checkpoints')\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\nbest_val_acc = 0.0\nbest_model_path = SAVE_DIR / f'efficientnet_v2s_{ACTIVE_DATASET}_v1.pt'\n\nhistory = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'lr': []}\n\nprint('=== Phase 1: Frozen Backbone ===')\nprint(f'Training classifier head only ({count_parameters(model)[0]:,} params)')\nprint(f'Checkpoint will save to: {best_model_path}\\n')\n\nfor epoch in range(EPOCHS_PHASE1):\n    t0 = time.time()\n    \n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n    \n    scheduler.step()\n    current_lr = scheduler.get_last_lr()[0]\n    elapsed = time.time() - t0\n    \n    # Track history\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['train_acc'].append(train_acc)\n    history['val_acc'].append(val_acc)\n    history['lr'].append(current_lr)\n    \n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'val_acc': val_acc,\n            'val_loss': val_loss,\n            'class_names': CLASS_NAMES,\n            'num_classes': NUM_CLASSES,\n            'img_size': IMG_SIZE,\n            'dataset': ACTIVE_DATASET,\n        }, best_model_path)\n        marker = ' * (best)'\n    else:\n        marker = ''\n    \n    print(f'Epoch [{epoch+1:2d}/{EPOCHS_PHASE1}] '\n          f'train_loss={train_loss:.4f} train_acc={train_acc:.4f} | '\n          f'val_loss={val_loss:.4f} val_acc={val_acc:.4f} | '\n          f'lr={current_lr:.6f} | {elapsed:.1f}s{marker}')\n    \n    # W&B logging\n    if USE_WANDB:\n        wandb.log({\n            'epoch': epoch + 1,\n            'phase': 1,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'train_acc': train_acc,\n            'val_acc': val_acc,\n            'lr': current_lr,\n        })\n\nprint(f'\\nPhase 1 complete. Best val accuracy: {best_val_acc:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 2: Unfreeze Backbone (Fine-Tuning)\n\nNow we unfreeze the entire model and train with a lower learning rate (1e-4). This lets the backbone adapt its features to weed-specific patterns.\n\n**Why lower LR?** The backbone weights are already well-trained on ImageNet. We want small, careful updates \u2014 large updates would destroy the learned features (\"catastrophic forgetting\").\n\n**When to skip Phase 2:** If Phase 1 accuracy is already >85%, Phase 2 may not add much. But running it is still recommended for best results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Phase 2: Unfreeze and fine-tune ---\nLR_PHASE2 = 1e-4\nEPOCHS_PHASE2 = 10\n\nunfreeze_all(model)\ntrainable, total = count_parameters(model)\n\n# New optimizer for all parameters\noptimizer = optim.Adam(model.parameters(), lr=LR_PHASE2)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_PHASE2)\n\nprint('=== Phase 2: Full Fine-Tuning ===')\nprint(f'All parameters unfrozen ({trainable:,} trainable)\\n')\n\nfor epoch in range(EPOCHS_PHASE2):\n    t0 = time.time()\n    global_epoch = EPOCHS_PHASE1 + epoch\n    \n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\n    \n    scheduler.step()\n    current_lr = scheduler.get_last_lr()[0]\n    elapsed = time.time() - t0\n    \n    # Track history\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['train_acc'].append(train_acc)\n    history['val_acc'].append(val_acc)\n    history['lr'].append(current_lr)\n    \n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save({\n            'epoch': global_epoch,\n            'model_state_dict': model.state_dict(),\n            'val_acc': val_acc,\n            'val_loss': val_loss,\n            'class_names': CLASS_NAMES,\n            'num_classes': NUM_CLASSES,\n            'img_size': IMG_SIZE,\n        }, best_model_path)\n        marker = ' * (best)'\n    else:\n        marker = ''\n    \n    print(f'Epoch [{global_epoch+1:2d}/{EPOCHS_PHASE1 + EPOCHS_PHASE2}] '\n          f'train_loss={train_loss:.4f} train_acc={train_acc:.4f} | '\n          f'val_loss={val_loss:.4f} val_acc={val_acc:.4f} | '\n          f'lr={current_lr:.6f} | {elapsed:.1f}s{marker}')\n    \n    # W&B logging\n    if USE_WANDB:\n        wandb.log({\n            'epoch': global_epoch + 1,\n            'phase': 2,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'train_acc': train_acc,\n            'val_acc': val_acc,\n            'lr': current_lr,\n        })\n\nprint(f'\\nPhase 2 complete. Best val accuracy: {best_val_acc:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Training History Plot\n\nVisualize how loss and accuracy evolved across both phases."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\nepochs_range = range(1, len(history['train_loss']) + 1)\nphase_boundary = EPOCHS_PHASE1\n\n# Loss\naxes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train')\naxes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val')\naxes[0].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Loss')\naxes[0].legend()\n\n# Accuracy\naxes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train')\naxes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val')\naxes[1].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_title('Accuracy')\naxes[1].legend()\n\n# Learning rate\naxes[2].plot(epochs_range, history['lr'], 'g-')\naxes[2].axvline(x=phase_boundary, color='gray', linestyle='--', alpha=0.5, label='Unfreeze')\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Learning Rate')\naxes[2].set_title('Learning Rate Schedule')\naxes[2].legend()\n\nplt.suptitle('Training History (Phase 1: frozen -> Phase 2: unfrozen)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Evaluation\n\nLoad the best checkpoint and run a thorough evaluation:\n- Confusion matrix\n- Per-class precision, recall, F1\n- Correctly classified and misclassified examples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model\ncheckpoint = torch.load(best_model_path, map_location=DEVICE, weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f'Loaded best model from epoch {checkpoint[\"epoch\"] + 1}')\nprint(f'Best val accuracy: {checkpoint[\"val_acc\"]:.4f}')\nprint(f'Best val loss: {checkpoint[\"val_loss\"]:.4f}')\n\n# Run validation to get final predictions\nval_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, DEVICE)\nprint(f'\\nFinal validation: loss={val_loss:.4f}, accuracy={val_acc:.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Confusion Matrix\n\nThe confusion matrix shows which classes the model confuses. Diagonal entries = correct predictions. Off-diagonal = mistakes.\n\n**What to look for:**\n- Large off-diagonal values = classes the model confuses (visually similar?)\n- A row with low diagonal value = a class the model struggles to recognize\n- A column with many values = a class the model over-predicts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix\ncm = confusion_matrix(val_labels, val_preds)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\nfig, axes = plt.subplots(1, 2, figsize=(20, 8))\n\n# Absolute counts\nim0 = axes[0].imshow(cm, interpolation='nearest', cmap='Blues')\naxes[0].set_title('Confusion Matrix (Counts)')\nfor i in range(NUM_CLASSES):\n    for j in range(NUM_CLASSES):\n        axes[0].text(j, i, str(cm[i, j]), ha='center', va='center',\n                     color='white' if cm[i, j] > cm.max() / 2 else 'black', fontsize=8)\n\n# Normalized (recall per class)\nim1 = axes[1].imshow(cm_normalized, interpolation='nearest', cmap='Blues', vmin=0, vmax=1)\naxes[1].set_title('Confusion Matrix (Normalized by Row = Recall)')\nfor i in range(NUM_CLASSES):\n    for j in range(NUM_CLASSES):\n        axes[1].text(j, i, f'{cm_normalized[i, j]:.2f}', ha='center', va='center',\n                     color='white' if cm_normalized[i, j] > 0.5 else 'black', fontsize=8)\n\nfor ax in axes:\n    ax.set_xticks(range(NUM_CLASSES))\n    ax.set_yticks(range(NUM_CLASSES))\n    ax.set_xticklabels(CLASS_LIST, rotation=45, ha='right', fontsize=9)\n    ax.set_yticklabels(CLASS_LIST, fontsize=9)\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n\nfig.colorbar(im0, ax=axes[0], fraction=0.046)\nfig.colorbar(im1, ax=axes[1], fraction=0.046)\nplt.suptitle('Classification Results', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Log to W&B\nif USE_WANDB:\n    wandb.log({'confusion_matrix': wandb.Image(fig)})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-class metrics\nreport = classification_report(\n    val_labels, val_preds, \n    target_names=CLASS_LIST, \n    digits=3,\n    output_dict=True\n)\n\nprint('Per-Class Classification Report:')\nprint('=' * 70)\nprint(classification_report(val_labels, val_preds, target_names=CLASS_LIST, digits=3))\n\n# Visual: F1 per class\nf1_scores = [report[name]['f1-score'] for name in CLASS_LIST]\ncolors = ['green' if f1 > 0.8 else 'orange' if f1 > 0.6 else 'red' for f1 in f1_scores]\n\nfig, ax = plt.subplots(figsize=(12, 5))\nbars = ax.barh(CLASS_LIST, f1_scores, color=colors)\nax.set_xlabel('F1 Score')\nax.set_title('Per-Class F1 Score')\nax.set_xlim(0, 1)\nax.axvline(x=0.8, color='gray', linestyle='--', alpha=0.5, label='Good threshold')\n\nfor bar, f1 in zip(bars, f1_scores):\n    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n            f'{f1:.3f}', va='center', fontsize=10)\n\nax.legend()\nplt.tight_layout()\nplt.show()\n\n# Log to W&B\nif USE_WANDB:\n    wandb.log({'f1_per_class': wandb.Image(fig)})\n    for name in CLASS_LIST:\n        wandb.log({f'f1_{name}': report[name]['f1-score']})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Correct vs Misclassified Examples\n\nLooking at what the model gets right and wrong is more informative than any metric. Pay attention to:\n- **Misclassified images:** Are they genuinely ambiguous? Mislabeled? Or a clear model failure?\n- **Which classes get confused?** Cross-reference with the confusion matrix."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect predictions for individual images\ncorrect_indices = np.where(val_preds == val_labels)[0]\nincorrect_indices = np.where(val_preds != val_labels)[0]\n\nprint(f'Correct:   {len(correct_indices):,} ({len(correct_indices)/len(val_labels)*100:.1f}%)')\nprint(f'Incorrect: {len(incorrect_indices):,} ({len(incorrect_indices)/len(val_labels)*100:.1f}%)')\n\n\ndef show_predictions(indices, title, n=5):\n    \"\"\"Show predicted vs actual labels for a set of image indices.\"\"\"\n    np.random.seed(42)\n    selected = np.random.choice(indices, size=min(n, len(indices)), replace=False)\n    \n    fig, axes = plt.subplots(1, n, figsize=(3 * n, 4))\n    if n == 1:\n        axes = [axes]\n    \n    for ax, idx in zip(axes, selected):\n        img_path = val_df.iloc[idx]['image_path']\n        img = Image.open(img_path).convert('RGB')\n        ax.imshow(img)\n        \n        pred_name = CLASS_NAMES[val_preds[idx]]\n        true_name = CLASS_NAMES[val_labels[idx]]\n        \n        if val_preds[idx] == val_labels[idx]:\n            ax.set_title(f'Pred: {pred_name}\\nTrue: {true_name}', fontsize=9, color='green')\n        else:\n            ax.set_title(f'Pred: {pred_name}\\nTrue: {true_name}', fontsize=9, color='red')\n        ax.axis('off')\n    \n    plt.suptitle(title, fontsize=13, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\n\nshow_predictions(correct_indices, 'Correctly Classified Examples', n=5)\nshow_predictions(incorrect_indices, 'Misclassified Examples', n=5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. Export Model\n\nSave the final model checkpoint for use in notebook 05 (inference pipeline)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final model info\nmodel_size_mb = best_model_path.stat().st_size / (1024 * 1024)\n\nprint(f'=== Model Export Summary ===')\nprint(f'Model:          EfficientNetV2-S')\nprint(f'Dataset:        {config[\"name\"]} ({NUM_CLASSES} classes)')\nprint(f'Best val acc:   {best_val_acc:.4f} ({best_val_acc*100:.1f}%)')\nprint(f'Checkpoint:     {best_model_path}')\nprint(f'Model size:     {model_size_mb:.1f} MB')\nprint(f'Input size:     {IMG_SIZE}x{IMG_SIZE} RGB')\nprint(f'Normalization:  ImageNet (mean={IMAGENET_MEAN}, std={IMAGENET_STD})')\nprint(f'\\nCheckpoint contents:')\nfor key in checkpoint.keys():\n    print(f'  - {key}')\n\n# Log final summary to W&B\nif USE_WANDB:\n    wandb.summary['best_val_acc'] = best_val_acc\n    wandb.summary['model_size_mb'] = model_size_mb\n    wandb.summary['dataset'] = ACTIVE_DATASET\n    wandb.save(str(best_model_path))\n    wandb.finish()\n    print(f'\\nW&B run finished. Model artifact saved.')\n\nprint(f'\\n=== Next Steps ===')\nprint(f'1. Download {best_model_path.name} from Kaggle output tab (if needed locally)')\nprint(f'2. Notebook 05: Build inference pipeline using this checkpoint')\nprint(f'3. Notebook 02: Explore RiceSEG for segmentation training')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}